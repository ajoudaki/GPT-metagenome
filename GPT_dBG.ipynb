{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xccSsJlVtbf-",
    "outputId": "e7ad6f44-a182-46c6-831a-1ef4c225a05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RA': 0.0060596996996997, 'AI': 0.0027986186186186187, 'IG': 0.0022578778778778777, 'GA': 0.004134294294294294, 'AP': 0.0036618418418418418, 'PI': 0.0020397797797797796, 'IT': 0.0029932532532532534, 'TR': 0.005234774774774775, 'R*': 0.004365665665665665, '*I': 0.0025149549549549548, 'II': 0.0031766166166166164, 'IK': 0.0025243643643643645, 'K*': 0.0018204604604604604, '*L': 0.0037794794794794796, 'LT': 0.004996996996996997, 'RN': 0.0029564964964964966, 'NN': 0.0015886486486486487, 'NV': 0.0019713313313313315, 'VS': 0.004523563563563563, 'SL': 0.007338418418418418, 'LF': 0.003673153153153153, 'F*': 0.0018854854854854855, '*R': 0.004338218218218218, 'RD': 0.002632112112112112, 'DR': 0.0029824624624624624, 'RI': 0.003700660660660661, 'IF': 0.0024567567567567567, 'FP': 0.001611831831831832, 'PF': 0.00166996996996997, 'FF': 0.002012752752752753, 'FK': 0.0016756356356356357, 'KE': 0.0017542742742742742, 'EL': 0.003033333333333333, 'LK': 0.0040688288288288285, 'KQ': 0.0015731531531531532, 'QR': 0.0035291891891891893, 'RV': 0.00456032032032032, 'VG': 0.003364864864864865, 'AC': 0.0016073273273273273, 'CC': 0.001344084084084084, 'CK': 0.000995895895895896, 'KN': 0.0019625625625625627, 'NR': 0.0025593993993993994, 'RK': 0.0033963763763763762, 'Q*': 0.0014118318318318318, '*E': 0.0011983383383383384, 'EY': 0.0010643043043043043, 'YS': 0.0024047647647647647, 'SS': 0.009883723723723724, 'S*': 0.0038477477477477477, '*K': 0.002276856856856857, 'KC': 0.0009577377377377377, 'CF': 0.0012518118118118118, 'FS': 0.0033605205205205204, 'SE': 0.002397117117117117, 'EG': 0.0020467267267267266, 'GY': 0.001642082082082082, 'Y*': 0.0015290890890890892, '*Y': 0.0014490690690690691, 'YC': 0.0008974374374374375, 'CY': 0.0008819019019019019, 'YN': 0.0011378778778778778, 'NE': 0.0012235235235235236, 'EQ': 0.0013858658658658658, 'QS': 0.0026234034034034034, '*D': 0.0009199199199199199, 'DT': 0.0016658658658658659, 'TE': 0.0018794394394394394, 'EW': 0.0005216416416416416, 'WK': 0.0007668868868868869, 'KS': 0.003358578578578579, 'SV': 0.004663563563563564, 'VY': 0.001598978978978979, 'YF': 0.001409029029029029, 'FR': 0.002715295295295295, 'RR': 0.012933613613613613, 'RF': 0.0029063863863863864, '*C': 0.001499019019019019, 'FG': 0.0019472872872872873, 'GH': 0.0017602002002002001, 'HL': 0.002833993993993994, 'FW': 0.0006308508508508509, 'W*': 0.0009265065065065066, 'GN': 0.0019654454454454454, 'RS': 0.009072332332332332, 'SI': 0.004402902902902903, 'RY': 0.002404664664664665, 'IQ': 0.001802142142142142, 'QA': 0.0022716716716716717, 'AS': 0.005790790790790791, 'SY': 0.0022866066066066066, '*A': 0.0018906306306306305, 'AE': 0.0024892292292292293, 'ER': 0.00302984984984985, 'QL': 0.0031897097097097096, 'LE': 0.0029114514514514515, 'EK': 0.0016263663663663665, '*F': 0.0016918118118118119, 'FT': 0.0020734334334334336, 'TC': 0.0016675875875875876, 'CW': 0.0007266466466466467, 'KW': 0.0006432232232232233, '**': 0.0027715315315315317, '*G': 0.001857077077077077, 'GE': 0.0021654854854854856, 'EF': 0.0011817617617617617, 'FH': 0.001154994994994995, 'HH': 0.0014219219219219219, 'HK': 0.000891011011011011, 'YA': 0.0014164364364364363, 'AN': 0.0018706106106106107, 'NM': 0.0006045845845845845, 'ME': 0.0006659059059059059, 'A*': 0.001980780780780781, 'IH': 0.001425905905905906, 'HA': 0.0015964564564564564, 'AH': 0.001576896896896897, 'HI': 0.0012404604604604604, 'KT': 0.0025872072072072073, 'TH': 0.0014022222222222222, 'HN': 0.000843983983983984, 'NL': 0.0032775775775775776, 'LG': 0.0042730930930930935, 'GC': 0.0018795195195195195, 'CP': 0.0014355555555555556, 'PY': 0.0011766166166166166, 'NP': 0.001713853853853854, 'P*': 0.001965865865865866, '*H': 0.0011967367367367368, 'H*': 0.0013003203203203204, 'QC': 0.0008131931931931932, 'C*': 0.0016963563563563563, '*N': 0.001843023023023023, 'NY': 0.0012010410410410411, 'FD': 0.0012948148148148148, 'D*': 0.0010435635635635636, '*P': 0.002286326326326326, 'PK': 0.0017302902902902903, 'QN': 0.001285085085085085, 'ND': 0.0011968568568568568, 'DQ': 0.0011833433433433435, 'RE': 0.0026596196196196197, 'EN': 0.0013164764764764766, 'NT': 0.0020632032032032034, 'QT': 0.0019607207207207205, 'TM': 0.0010115115115115114, 'ML': 0.0016673873873873874, 'LI': 0.004672272272272272, 'KR': 0.0033484684684684683, 'YV': 0.0014552152152152152, 'VF': 0.0022676276276276277, 'FY': 0.001336836836836837, 'YL': 0.002837957957957958, 'LP': 0.0050294094094094095, 'PS': 0.005173973973973974, 'LV': 0.005239599599599599, 'VI': 0.0028294694694694696, 'IP': 0.0023196996996997, 'PE': 0.0020331331331331333, 'KK': 0.0025071871871871873, 'KH': 0.0011337137137137137, 'HS': 0.002173093093093093, 'SW': 0.0016758358358358357, 'WA': 0.0009075475475475475, 'AV': 0.004021901901901902, 'VH': 0.0016824224224224224, 'HE': 0.0009104904904904905, 'EA': 0.0023514514514514514, '*S': 0.003874774774774775, 'SD': 0.002433133133133133, 'EI': 0.001815075075075075, 'IC': 0.0014287687687687687, 'LC': 0.0023057857857857857, 'CL': 0.0024402002002002, 'CH': 0.0008297497497497498, 'HV': 0.0014942742742742744, 'FL': 0.003942022022022022, 'EE': 0.0017016016016016015, 'LL': 0.009497977977977978, 'LR': 0.007398818818818819, 'CI': 0.001338098098098098, 'IR': 0.0035436436436436436, 'L*': 0.004014854854854854, 'CA': 0.0017047647647647649, 'AY': 0.0014151351351351352, 'YQ': 0.001236956956956957, 'IN': 0.0022024424424424423, 'SM': 0.0014563963963963964, 'MQ': 0.0006025425425425425, 'QV': 0.0020265265265265267, 'VN': 0.0019689089089089088, 'T*': 0.002223983983983984, 'N*': 0.001627847847847848, '*W': 0.0008191991991991992, 'WE': 0.000446006006006006, 'ES': 0.0024014414414414414, 'SN': 0.0029542942942942943, 'NS': 0.0029606806806806805, 'FA': 0.0018935535535535535, 'SC': 0.002753973973973974, 'CR': 0.0030714114114114115, 'KA': 0.0023333933933933934, 'SF': 0.0034367967967967967, 'RT': 0.005551691691691692, 'YR': 0.002368688688688689, 'DG': 0.0025522722722722724, 'G*': 0.0023555755755755754, 'RL': 0.007446486486486486, 'IL': 0.0047936936936936935, 'LS': 0.007176636636636637, 'LN': 0.0032208608608608608, 'YI': 0.00160008008008008, 'IY': 0.0017393393393393393, 'QP': 0.002098858858858859, 'FE': 0.001191831831831832, 'E*': 0.0010884284284284284, 'KF': 0.0015467667667667668, 'IM': 0.0009081281281281281, 'MI': 0.0009717717717717718, 'VC': 0.0015531731731731732, 'CV': 0.0014153553553553553, 'FV': 0.002137397397397397, 'IV': 0.002651071071071071, 'VL': 0.005353913913913914, 'FI': 0.002392872872872873, 'NK': 0.0016992392392392393, 'KI': 0.0025454054054054056, 'IS': 0.004433433433433434, 'SK': 0.003335235235235235, '*T': 0.002667247247247247, 'TK': 0.0024346746746746746, 'QK': 0.0016251851851851852, 'TI': 0.0030264464464464466, 'I*': 0.0025023223223223224, 'NF': 0.0015637437437437437, 'GF': 0.0020362962962962965, 'SA': 0.00609997997997998, 'AF': 0.002015855855855856, 'WI': 0.0008111311311311311, 'TF': 0.0021653253253253253, 'KY': 0.0013761161161161162, 'NH': 0.0011344744744744744, 'GI': 0.002572212212212212, 'PD': 0.0020276276276276275, 'DL': 0.0027755155155155154, 'CN': 0.0009519119119119119, 'EH': 0.0010392192192192192, 'LW': 0.0013575375375375375, 'QW': 0.0005193793793793794, 'WH': 0.00048604604604604606, 'LH': 0.0027754954954954955, 'MS': 0.001314014014014014, 'YT': 0.0014635235235235236, 'YM': 0.0004337337337337337, 'PR': 0.005914634634634635, 'LD': 0.002803123123123123, 'KL': 0.0038181981981981984, 'PN': 0.001475995995995996, 'VP': 0.0029227427427427426, 'SP': 0.005168268268268268, 'NW': 0.0005921321321321322, '*M': 0.0007830430430430431, 'MY': 0.0004621221221221221, 'DV': 0.0020643843843843844, 'HF': 0.0010473473473473474, 'YH': 0.0009513713713713714, 'HQ': 0.0015094694694694694, 'LY': 0.002708868868868869, 'LQ': 0.003383743743743744, 'HP': 0.0018232232232232232, 'PP': 0.0036334334334334334, 'EC': 0.0007296296296296297, 'PL': 0.004243423423423424, 'DP': 0.0017206806806806807, 'GL': 0.0047037437437437435, 'TS': 0.006057837837837838, 'KP': 0.001995975975975976, 'EM': 0.0006219219219219219, 'MF': 0.0006276276276276277, 'ID': 0.00166, 'QI': 0.001731971971971972, 'YP': 0.0012935335335335335, 'LM': 0.0018341941941941943, 'MK': 0.0009487487487487488, 'KM': 0.0008924124124124124, 'M*': 0.00057995995995996, 'NQ': 0.0013845445445445446, 'QF': 0.0011414214214214215, 'QH': 0.0012615215215215215, 'IE': 0.0017248448448448448, 'PQ': 0.0017197597597597598, 'AL': 0.00498936936936937, 'GS': 0.005061581581581581, 'PW': 0.0008677677677677677, 'PC': 0.0015266066066066065, 'CT': 0.0015254254254254255, 'TV': 0.0033238838838838838, 'YY': 0.0011123323323323324, 'SH': 0.0020014814814814817, 'WM': 0.00033405405405405406, 'RW': 0.001943983983983984, 'HC': 0.0008560760760760761, 'CM': 0.0003612412412412412, 'MC': 0.00034092092092092094, 'IA': 0.00263001001001001, 'QQ': 0.0016968168168168167, 'HY': 0.0008509309309309309, 'SR': 0.008242662662662662, 'ET': 0.0018447047047047047, 'VE': 0.0021916116116116116, 'ED': 0.001450950950950951, 'VQ': 0.0020437637637637637, 'SQ': 0.002478398398398398, 'V*': 0.002178318318318318, 'CS': 0.0030385985985985986, 'RC': 0.0033334534534534533, 'CE': 0.0006783183183183183, 'WT': 0.0010265265265265266, 'VD': 0.0022053453453453453, 'DS': 0.002371991991991992, 'FC': 0.001212872872872873, 'HG': 0.0018664864864864864, 'NA': 0.0019887887887887886, 'HR': 0.003332792792792793, 'DN': 0.0011378778778778778, 'PT': 0.003565965965965966, '*V': 0.0020401201201201203, 'CQ': 0.0009171771771771771, 'WN': 0.0006234234234234234, 'NI': 0.0021952552552552554, 'TP': 0.0036743943943943944, 'RH': 0.002987027027027027, 'PH': 0.0015057257257257257, 'RG': 0.005555415415415416, 'LA': 0.005105765765765766, 'QE': 0.0013907707707707707, 'EV': 0.0021745345345345344, 'RP': 0.006207307307307308, 'GW': 0.0011343343343343344, 'WR': 0.0019633433433433435, 'GG': 0.004282582582582583, 'WL': 0.00162990990990991, 'QG': 0.002096096096096096, 'GP': 0.0027580980980980983, 'AQ': 0.0020491091091091092, 'PM': 0.0007604204204204204, 'MG': 0.0008270470470470471, 'VK': 0.0023021821821821824, 'SG': 0.005540560560560561, 'AR': 0.006264684684684684, 'QM': 0.0006407407407407408, 'MN': 0.0006785385385385385, 'QD': 0.0011572972972972973, 'DF': 0.0012757557557557557, 'YK': 0.0012261261261261262, 'TY': 0.0014978378378378379, 'MW': 0.0002305105105105105, 'AD': 0.0026065465465465467, 'HD': 0.0010016216216216216, 'WC': 0.0006905505505505505, 'NC': 0.000981021021021021, 'CD': 0.0007381981981981982, 'PG': 0.003893113113113113, 'HT': 0.0013354354354354354, 'TD': 0.0017918318318318317, 'GD': 0.0023371171171171173, 'DA': 0.0023845245245245244, 'GM': 0.0008885685685685686, 'MA': 0.0011776976976976977, 'AA': 0.00608962962962963, 'VA': 0.003717237237237237, 'GR': 0.00652008008008008, 'TL': 0.004857937937937938, 'AM': 0.0010522722722722724, 'MT': 0.0010885885885885886, 'YG': 0.0015642642642642642, 'KV': 0.0023123323323323325, 'FN': 0.0016021221221221222, 'TQ': 0.0017384184184184184, 'NG': 0.0021624024024024025, 'GQ': 0.002077037037037037, 'TN': 0.0021046846846846847, 'KD': 0.0013336936936936938, 'TT': 0.004512872872872873, 'AK': 0.00221013013013013, 'DI': 0.001600940940940941, 'VM': 0.001017877877877878, 'TA': 0.0043036836836836834, 'GV': 0.003591611611611612, 'HM': 0.00033851851851851854, 'VT': 0.0031166566566566568, 'ST': 0.0061971171171171175, 'MM': 0.00047555555555555556, 'AT': 0.004057817817817818, 'TG': 0.0036904504504504504, 'GT': 0.0032997197197197196, 'CG': 0.0018638038038038037, 'VV': 0.0038643643643643645, 'DY': 0.0010091891891891892, 'TW': 0.0009935535535535536, 'WF': 0.000657937937937938, 'RQ': 0.0032965565565565564, 'PA': 0.00467033033033033, 'MR': 0.0012142942942942943, 'DH': 0.0010422222222222222, 'KG': 0.001907967967967968, 'FQ': 0.0013626626626626628, 'DK': 0.0012764164164164164, 'WS': 0.0015417217217217217, 'DE': 0.0015702102102102102, 'YD': 0.0009595795795795795, 'PV': 0.003338918918918919, 'GK': 0.002191031031031031, 'MD': 0.0005697297297297297, 'DC': 0.000826006006006006, 'DD': 0.0016514114114114114, 'MP': 0.0008403003003003003, 'FM': 0.0005994794794794794, 'YE': 0.0009445245245245245, 'QY': 0.0009992792792792792, 'WV': 0.0008510310310310311, 'MH': 0.0003528928928928929, 'RM': 0.0013167367367367367, 'IW': 0.0008023623623623624, 'WD': 0.00041645645645645645, 'DM': 0.0005151751751751751, 'EP': 0.0017031031031031032, 'VR': 0.004783203203203203, 'AG': 0.005105565565565565, 'MV': 0.0012162362362362362, 'WQ': 0.0006126126126126127, 'DW': 0.0005380980980980981, 'AW': 0.0010724524524524525, 'WY': 0.0005234034034034034, 'WP': 0.001031851851851852, 'YW': 0.0004928728728728728, 'VW': 0.000932912912912913, 'HW': 0.00045435435435435437, 'WW': 0.0004635835835835836, 'WG': 0.0007017617617617618, '*Q': 0.0014182582582582582, 'KX': 2.002002002002002e-08}\n",
      "entropy =  4.237556093216438\n",
      "{'RA': 0.0060596996996997, 'AI': 0.0027986186186186187, 'IG': 0.0022578778778778777, 'GA': 0.004134294294294294, 'AP': 0.0036618418418418418, 'PI': 0.0020397797797797796, 'IT': 0.0029932532532532534, 'TR': 0.005234774774774775, 'R*': 0.004365665665665665, '*I': 0.0025149549549549548, 'II': 0.0031766166166166164, 'IK': 0.0025243643643643645, 'K*': 0.0018204604604604604, '*L': 0.0037794794794794796, 'LT': 0.004996996996996997, 'RN': 0.0029564964964964966, 'NN': 0.0015886486486486487, 'NV': 0.0019713313313313315, 'VS': 0.004523563563563563, 'SL': 0.007338418418418418, 'LF': 0.003673153153153153, 'F*': 0.0018854854854854855, '*R': 0.004338218218218218, 'RD': 0.002632112112112112, 'DR': 0.0029824624624624624, 'RI': 0.003700660660660661, 'IF': 0.0024567567567567567, 'FP': 0.001611831831831832, 'PF': 0.00166996996996997, 'FF': 0.002012752752752753, 'FK': 0.0016756356356356357, 'KE': 0.0017542742742742742, 'EL': 0.003033333333333333, 'LK': 0.0040688288288288285, 'KQ': 0.0015731531531531532, 'QR': 0.0035291891891891893, 'RV': 0.00456032032032032, 'VG': 0.003364864864864865, 'AC': 0.0016073273273273273, 'CC': 0.001344084084084084, 'CK': 0.000995895895895896, 'KN': 0.0019625625625625627, 'NR': 0.0025593993993993994, 'RK': 0.0033963763763763762, 'Q*': 0.0014118318318318318, '*E': 0.0011983383383383384, 'EY': 0.0010643043043043043, 'YS': 0.0024047647647647647, 'SS': 0.009883723723723724, 'S*': 0.0038477477477477477, '*K': 0.002276856856856857, 'KC': 0.0009577377377377377, 'CF': 0.0012518118118118118, 'FS': 0.0033605205205205204, 'SE': 0.002397117117117117, 'EG': 0.0020467267267267266, 'GY': 0.001642082082082082, 'Y*': 0.0015290890890890892, '*Y': 0.0014490690690690691, 'YC': 0.0008974374374374375, 'CY': 0.0008819019019019019, 'YN': 0.0011378778778778778, 'NE': 0.0012235235235235236, 'EQ': 0.0013858658658658658, 'QS': 0.0026234034034034034, '*D': 0.0009199199199199199, 'DT': 0.0016658658658658659, 'TE': 0.0018794394394394394, 'EW': 0.0005216416416416416, 'WK': 0.0007668868868868869, 'KS': 0.003358578578578579, 'SV': 0.004663563563563564, 'VY': 0.001598978978978979, 'YF': 0.001409029029029029, 'FR': 0.002715295295295295, 'RR': 0.012933613613613613, 'RF': 0.0029063863863863864, '*C': 0.001499019019019019, 'FG': 0.0019472872872872873, 'GH': 0.0017602002002002001, 'HL': 0.002833993993993994, 'FW': 0.0006308508508508509, 'W*': 0.0009265065065065066, 'GN': 0.0019654454454454454, 'RS': 0.009072332332332332, 'SI': 0.004402902902902903, 'RY': 0.002404664664664665, 'IQ': 0.001802142142142142, 'QA': 0.0022716716716716717, 'AS': 0.005790790790790791, 'SY': 0.0022866066066066066, '*A': 0.0018906306306306305, 'AE': 0.0024892292292292293, 'ER': 0.00302984984984985, 'QL': 0.0031897097097097096, 'LE': 0.0029114514514514515, 'EK': 0.0016263663663663665, '*F': 0.0016918118118118119, 'FT': 0.0020734334334334336, 'TC': 0.0016675875875875876, 'CW': 0.0007266466466466467, 'KW': 0.0006432232232232233, '**': 0.0027715315315315317, '*G': 0.001857077077077077, 'GE': 0.0021654854854854856, 'EF': 0.0011817617617617617, 'FH': 0.001154994994994995, 'HH': 0.0014219219219219219, 'HK': 0.000891011011011011, 'YA': 0.0014164364364364363, 'AN': 0.0018706106106106107, 'NM': 0.0006045845845845845, 'ME': 0.0006659059059059059, 'A*': 0.001980780780780781, 'IH': 0.001425905905905906, 'HA': 0.0015964564564564564, 'AH': 0.001576896896896897, 'HI': 0.0012404604604604604, 'KT': 0.0025872072072072073, 'TH': 0.0014022222222222222, 'HN': 0.000843983983983984, 'NL': 0.0032775775775775776, 'LG': 0.0042730930930930935, 'GC': 0.0018795195195195195, 'CP': 0.0014355555555555556, 'PY': 0.0011766166166166166, 'NP': 0.001713853853853854, 'P*': 0.001965865865865866, '*H': 0.0011967367367367368, 'H*': 0.0013003203203203204, 'QC': 0.0008131931931931932, 'C*': 0.0016963563563563563, '*N': 0.001843023023023023, 'NY': 0.0012010410410410411, 'FD': 0.0012948148148148148, 'D*': 0.0010435635635635636, '*P': 0.002286326326326326, 'PK': 0.0017302902902902903, 'QN': 0.001285085085085085, 'ND': 0.0011968568568568568, 'DQ': 0.0011833433433433435, 'RE': 0.0026596196196196197, 'EN': 0.0013164764764764766, 'NT': 0.0020632032032032034, 'QT': 0.0019607207207207205, 'TM': 0.0010115115115115114, 'ML': 0.0016673873873873874, 'LI': 0.004672272272272272, 'KR': 0.0033484684684684683, 'YV': 0.0014552152152152152, 'VF': 0.0022676276276276277, 'FY': 0.001336836836836837, 'YL': 0.002837957957957958, 'LP': 0.0050294094094094095, 'PS': 0.005173973973973974, 'LV': 0.005239599599599599, 'VI': 0.0028294694694694696, 'IP': 0.0023196996996997, 'PE': 0.0020331331331331333, 'KK': 0.0025071871871871873, 'KH': 0.0011337137137137137, 'HS': 0.002173093093093093, 'SW': 0.0016758358358358357, 'WA': 0.0009075475475475475, 'AV': 0.004021901901901902, 'VH': 0.0016824224224224224, 'HE': 0.0009104904904904905, 'EA': 0.0023514514514514514, '*S': 0.003874774774774775, 'SD': 0.002433133133133133, 'EI': 0.001815075075075075, 'IC': 0.0014287687687687687, 'LC': 0.0023057857857857857, 'CL': 0.0024402002002002, 'CH': 0.0008297497497497498, 'HV': 0.0014942742742742744, 'FL': 0.003942022022022022, 'EE': 0.0017016016016016015, 'LL': 0.009497977977977978, 'LR': 0.007398818818818819, 'CI': 0.001338098098098098, 'IR': 0.0035436436436436436, 'L*': 0.004014854854854854, 'CA': 0.0017047647647647649, 'AY': 0.0014151351351351352, 'YQ': 0.001236956956956957, 'IN': 0.0022024424424424423, 'SM': 0.0014563963963963964, 'MQ': 0.0006025425425425425, 'QV': 0.0020265265265265267, 'VN': 0.0019689089089089088, 'T*': 0.002223983983983984, 'N*': 0.001627847847847848, '*W': 0.0008191991991991992, 'WE': 0.000446006006006006, 'ES': 0.0024014414414414414, 'SN': 0.0029542942942942943, 'NS': 0.0029606806806806805, 'FA': 0.0018935535535535535, 'SC': 0.002753973973973974, 'CR': 0.0030714114114114115, 'KA': 0.0023333933933933934, 'SF': 0.0034367967967967967, 'RT': 0.005551691691691692, 'YR': 0.002368688688688689, 'DG': 0.0025522722722722724, 'G*': 0.0023555755755755754, 'RL': 0.007446486486486486, 'IL': 0.0047936936936936935, 'LS': 0.007176636636636637, 'LN': 0.0032208608608608608, 'YI': 0.00160008008008008, 'IY': 0.0017393393393393393, 'QP': 0.002098858858858859, 'FE': 0.001191831831831832, 'E*': 0.0010884284284284284, 'KF': 0.0015467667667667668, 'IM': 0.0009081281281281281, 'MI': 0.0009717717717717718, 'VC': 0.0015531731731731732, 'CV': 0.0014153553553553553, 'FV': 0.002137397397397397, 'IV': 0.002651071071071071, 'VL': 0.005353913913913914, 'FI': 0.002392872872872873, 'NK': 0.0016992392392392393, 'KI': 0.0025454054054054056, 'IS': 0.004433433433433434, 'SK': 0.003335235235235235, '*T': 0.002667247247247247, 'TK': 0.0024346746746746746, 'QK': 0.0016251851851851852, 'TI': 0.0030264464464464466, 'I*': 0.0025023223223223224, 'NF': 0.0015637437437437437, 'GF': 0.0020362962962962965, 'SA': 0.00609997997997998, 'AF': 0.002015855855855856, 'WI': 0.0008111311311311311, 'TF': 0.0021653253253253253, 'KY': 0.0013761161161161162, 'NH': 0.0011344744744744744, 'GI': 0.002572212212212212, 'PD': 0.0020276276276276275, 'DL': 0.0027755155155155154, 'CN': 0.0009519119119119119, 'EH': 0.0010392192192192192, 'LW': 0.0013575375375375375, 'QW': 0.0005193793793793794, 'WH': 0.00048604604604604606, 'LH': 0.0027754954954954955, 'MS': 0.001314014014014014, 'YT': 0.0014635235235235236, 'YM': 0.0004337337337337337, 'PR': 0.005914634634634635, 'LD': 0.002803123123123123, 'KL': 0.0038181981981981984, 'PN': 0.001475995995995996, 'VP': 0.0029227427427427426, 'SP': 0.005168268268268268, 'NW': 0.0005921321321321322, '*M': 0.0007830430430430431, 'MY': 0.0004621221221221221, 'DV': 0.0020643843843843844, 'HF': 0.0010473473473473474, 'YH': 0.0009513713713713714, 'HQ': 0.0015094694694694694, 'LY': 0.002708868868868869, 'LQ': 0.003383743743743744, 'HP': 0.0018232232232232232, 'PP': 0.0036334334334334334, 'EC': 0.0007296296296296297, 'PL': 0.004243423423423424, 'DP': 0.0017206806806806807, 'GL': 0.0047037437437437435, 'TS': 0.006057837837837838, 'KP': 0.001995975975975976, 'EM': 0.0006219219219219219, 'MF': 0.0006276276276276277, 'ID': 0.00166, 'QI': 0.001731971971971972, 'YP': 0.0012935335335335335, 'LM': 0.0018341941941941943, 'MK': 0.0009487487487487488, 'KM': 0.0008924124124124124, 'M*': 0.00057995995995996, 'NQ': 0.0013845445445445446, 'QF': 0.0011414214214214215, 'QH': 0.0012615215215215215, 'IE': 0.0017248448448448448, 'PQ': 0.0017197597597597598, 'AL': 0.00498936936936937, 'GS': 0.005061581581581581, 'PW': 0.0008677677677677677, 'PC': 0.0015266066066066065, 'CT': 0.0015254254254254255, 'TV': 0.0033238838838838838, 'YY': 0.0011123323323323324, 'SH': 0.0020014814814814817, 'WM': 0.00033405405405405406, 'RW': 0.001943983983983984, 'HC': 0.0008560760760760761, 'CM': 0.0003612412412412412, 'MC': 0.00034092092092092094, 'IA': 0.00263001001001001, 'QQ': 0.0016968168168168167, 'HY': 0.0008509309309309309, 'SR': 0.008242662662662662, 'ET': 0.0018447047047047047, 'VE': 0.0021916116116116116, 'ED': 0.001450950950950951, 'VQ': 0.0020437637637637637, 'SQ': 0.002478398398398398, 'V*': 0.002178318318318318, 'CS': 0.0030385985985985986, 'RC': 0.0033334534534534533, 'CE': 0.0006783183183183183, 'WT': 0.0010265265265265266, 'VD': 0.0022053453453453453, 'DS': 0.002371991991991992, 'FC': 0.001212872872872873, 'HG': 0.0018664864864864864, 'NA': 0.0019887887887887886, 'HR': 0.003332792792792793, 'DN': 0.0011378778778778778, 'PT': 0.003565965965965966, '*V': 0.0020401201201201203, 'CQ': 0.0009171771771771771, 'WN': 0.0006234234234234234, 'NI': 0.0021952552552552554, 'TP': 0.0036743943943943944, 'RH': 0.002987027027027027, 'PH': 0.0015057257257257257, 'RG': 0.005555415415415416, 'LA': 0.005105765765765766, 'QE': 0.0013907707707707707, 'EV': 0.0021745345345345344, 'RP': 0.006207307307307308, 'GW': 0.0011343343343343344, 'WR': 0.0019633433433433435, 'GG': 0.004282582582582583, 'WL': 0.00162990990990991, 'QG': 0.002096096096096096, 'GP': 0.0027580980980980983, 'AQ': 0.0020491091091091092, 'PM': 0.0007604204204204204, 'MG': 0.0008270470470470471, 'VK': 0.0023021821821821824, 'SG': 0.005540560560560561, 'AR': 0.006264684684684684, 'QM': 0.0006407407407407408, 'MN': 0.0006785385385385385, 'QD': 0.0011572972972972973, 'DF': 0.0012757557557557557, 'YK': 0.0012261261261261262, 'TY': 0.0014978378378378379, 'MW': 0.0002305105105105105, 'AD': 0.0026065465465465467, 'HD': 0.0010016216216216216, 'WC': 0.0006905505505505505, 'NC': 0.000981021021021021, 'CD': 0.0007381981981981982, 'PG': 0.003893113113113113, 'HT': 0.0013354354354354354, 'TD': 0.0017918318318318317, 'GD': 0.0023371171171171173, 'DA': 0.0023845245245245244, 'GM': 0.0008885685685685686, 'MA': 0.0011776976976976977, 'AA': 0.00608962962962963, 'VA': 0.003717237237237237, 'GR': 0.00652008008008008, 'TL': 0.004857937937937938, 'AM': 0.0010522722722722724, 'MT': 0.0010885885885885886, 'YG': 0.0015642642642642642, 'KV': 0.0023123323323323325, 'FN': 0.0016021221221221222, 'TQ': 0.0017384184184184184, 'NG': 0.0021624024024024025, 'GQ': 0.002077037037037037, 'TN': 0.0021046846846846847, 'KD': 0.0013336936936936938, 'TT': 0.004512872872872873, 'AK': 0.00221013013013013, 'DI': 0.001600940940940941, 'VM': 0.001017877877877878, 'TA': 0.0043036836836836834, 'GV': 0.003591611611611612, 'HM': 0.00033851851851851854, 'VT': 0.0031166566566566568, 'ST': 0.0061971171171171175, 'MM': 0.00047555555555555556, 'AT': 0.004057817817817818, 'TG': 0.0036904504504504504, 'GT': 0.0032997197197197196, 'CG': 0.0018638038038038037, 'VV': 0.0038643643643643645, 'DY': 0.0010091891891891892, 'TW': 0.0009935535535535536, 'WF': 0.000657937937937938, 'RQ': 0.0032965565565565564, 'PA': 0.00467033033033033, 'MR': 0.0012142942942942943, 'DH': 0.0010422222222222222, 'KG': 0.001907967967967968, 'FQ': 0.0013626626626626628, 'DK': 0.0012764164164164164, 'WS': 0.0015417217217217217, 'DE': 0.0015702102102102102, 'YD': 0.0009595795795795795, 'PV': 0.003338918918918919, 'GK': 0.002191031031031031, 'MD': 0.0005697297297297297, 'DC': 0.000826006006006006, 'DD': 0.0016514114114114114, 'MP': 0.0008403003003003003, 'FM': 0.0005994794794794794, 'YE': 0.0009445245245245245, 'QY': 0.0009992792792792792, 'WV': 0.0008510310310310311, 'MH': 0.0003528928928928929, 'RM': 0.0013167367367367367, 'IW': 0.0008023623623623624, 'WD': 0.00041645645645645645, 'DM': 0.0005151751751751751, 'EP': 0.0017031031031031032, 'VR': 0.004783203203203203, 'AG': 0.005105565565565565, 'MV': 0.0012162362362362362, 'WQ': 0.0006126126126126127, 'DW': 0.0005380980980980981, 'AW': 0.0010724524524524525, 'WY': 0.0005234034034034034, 'WP': 0.001031851851851852, 'YW': 0.0004928728728728728, 'VW': 0.000932912912912913, 'HW': 0.00045435435435435437, 'WW': 0.0004635835835835836, 'WG': 0.0007017617617617618, '*Q': 0.0014182582582582582, 'KX': 2.002002002002002e-08}\n",
      "entropy =  4.237556093216438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirjoudaki\u001b[0m (\u001b[33msketch-bros\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/GPT_metagenome/wandb/run-20230614_172156-4gcswvw6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/4gcswvw6' target=\"_blank\">VIRAL_protein__lr=0.0001_bs=16_n_embed=1024_n_head=1_n_layer=4_early_stopping=5_num_seqs=50000_sequence_length=1000_stride=1000</a></strong> to <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/4gcswvw6' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/4gcswvw6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cffee3f6de42c6bb7e58d690252d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200 | Training:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, DefaultDict, Set\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "import wandb\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'GPT_dBG.ipynb'\n",
    "\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    HMM = \"HMM\"\n",
    "    VIRAL = \"viral\"\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.HMM\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.sequence_length: int = 1000\n",
    "        self.num_seqs: int = 500\n",
    "        self.num_hidden_states: int = 500\n",
    "        self.sparsity: float = 1.1\n",
    "        self.protein: bool = True\n",
    "        # self.stride: int = 1000\n",
    "        # self.split_ratio: float = 0.5\n",
    "        # self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        # self.substrings_per_seq: int = 20\n",
    "        # self.sequences_shuffle: bool = True\n",
    "        self.train_bs: int = 64\n",
    "        self.val_bs: int = 128\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 4\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 3\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 1000\n",
    "        self.stride: int = 1000\n",
    "        self.num_seqs: int = 50000\n",
    "        self.split_ratio: float = 0.2\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.protein: bool = True\n",
    "        self.sparsity: float = 1.1\n",
    "        self.num_hidden_states: int = 100\n",
    "        self.train_bs: int = 16\n",
    "        self.val_bs: int = 32\n",
    "        self.n_embed: int = 1024\n",
    "        self.n_layer: int = 6\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "        \n",
    "# DNA codon table\n",
    "dna_codon_table = {\n",
    "    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L',\n",
    "    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S',\n",
    "    'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*',\n",
    "    'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W',\n",
    "    'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n",
    "    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
    "    'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n",
    "    'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n",
    "    'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M',\n",
    "    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
    "    'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n",
    "    'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n",
    "    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n",
    "    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
    "    'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n",
    "    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'\n",
    "}\n",
    "\n",
    "\n",
    "def dna2protein(dna_sequence):\n",
    "    protein_dna = ''.join(dna_codon_table.get(dna_sequence[i:i+3], 'X') for i in range(0, len(dna_sequence), 3))\n",
    "    return protein_dna\n",
    "\n",
    "class SequenceTokenizer:\n",
    "    def __init__(self, protein: bool = False):\n",
    "        if protein:\n",
    "            self.alphabet = list(dna_codon_table.values())+['X']\n",
    "            self.alphabet = set(self.alphabet)\n",
    "        else:\n",
    "            self.alphabet = {'A', 'C', 'G', 'T', 'X'}\n",
    "        self.token_to_idx = {char: i for i, char in enumerate(self.alphabet)}\n",
    "        self.idx_to_token = {i: char for i, char in enumerate(self.alphabet)}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "\n",
    "    def encode(self, sequence: str, return_tensors: str = \"pt\") -> torch.Tensor:\n",
    "        tokens = [self.token_to_idx[char] for char in sequence]\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        sequence = [self.idx_to_token[int(token.item())] for token in tokens]\n",
    "        return ''.join(sequence)\n",
    "\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], tokenizer: SequenceTokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence = self.sequences[i]\n",
    "        inputs = self.tokenizer.encode(sequence, return_tensors='pt')\n",
    "        targets = inputs[1:].clone()\n",
    "        inputs = inputs[:-1]\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def compute_char_probabilities(sequences: List[str]) -> None:\n",
    "    counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    for seq in sequences:\n",
    "        cd = Counter(seq)\n",
    "        for c, count in cd.items():\n",
    "            counts[c] += count\n",
    "            total_count += count\n",
    "    pcounts = {char: c / total_count for char, c in counts.items()}\n",
    "    print(pcounts)\n",
    "\n",
    "def calculate_entropy(sequences, k):\n",
    "    substrings = [seq[i:i+k] for seq in sequences for i in range(len(seq) - k + 1)]\n",
    "    frequencies = Counter(substrings)\n",
    "    entropy = 0.0\n",
    "    total = sum(frequencies.values())\n",
    "    probabilities = {k: v / total for k, v in frequencies.items()}\n",
    "    for p in probabilities.values():\n",
    "        entropy -= p * math.log2(p)\n",
    "    print(probabilities)\n",
    "    print('entropy = ', entropy/k)\n",
    " \n",
    "\n",
    "def read_fna(file_path: str, shuffle: bool = False) -> List[str]:\n",
    "    sequences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"fasta\"):\n",
    "            sequences.append(str(record.seq))\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_markov_chain(n: int, sparsity: float) -> torch.Tensor:\n",
    "    transition_probs = torch.rand(n, n)\n",
    "    for i in range(n):\n",
    "        num_outgoing_states = torch.poisson(torch.tensor([float(sparsity)])).int().item()\n",
    "        num_outgoing_states = min(n, max(1, num_outgoing_states))\n",
    "        _, indices = torch.topk(transition_probs[i], int(num_outgoing_states))\n",
    "        mask = torch.zeros_like(transition_probs[i]).scatter_(0, indices, 1).to(torch.bool)\n",
    "        transition_probs[i] *= mask\n",
    "    transition_probs = F.normalize(transition_probs, p=1, dim=1)\n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "def draw_seq(transition_probs: torch.Tensor, sequence_length: int) -> str:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    current_state = 0\n",
    "    chain = [current_state]\n",
    "    while len(chain) < sequence_length:\n",
    "        next_state = torch.multinomial(transition_probs[int(current_state)], num_samples=1)\n",
    "        current_state = next_state.item()\n",
    "        chain.append(int(current_state))\n",
    "    seq = [NUCLEOTIDES[s % len(NUCLEOTIDES)] for s in chain]\n",
    "    return ''.join(seq)\n",
    "\n",
    "\n",
    "def generate_HMM_dataset(sequence_length: int, N: int, sparsity: float, num_hidden_states: int) -> List[List[str]]:\n",
    "    if num_hidden_states is None:\n",
    "        num_hidden_states = sequence_length\n",
    "    transition_matrix = generate_markov_chain(num_hidden_states, sparsity=sparsity)\n",
    "    all_seqs = [[], []]\n",
    "    for i in range(2):\n",
    "        seqs = []\n",
    "        while len(seqs) < N:\n",
    "            seq = draw_seq(transition_matrix, sequence_length)\n",
    "            if len(seq) >= sequence_length:\n",
    "                seqs.append(seq)\n",
    "        all_seqs[i] = seqs\n",
    "    return all_seqs\n",
    "\n",
    "\n",
    "def generate_phylo_dataset(sequence_length: int, mutation_rate: float, N: int) -> List[str]:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    parent_sequence = ''.join(random.choice(NUCLEOTIDES) for _ in range(sequence_length))\n",
    "    dataset = [parent_sequence]\n",
    "    for _ in range(N):\n",
    "        mutated_sequence = ''\n",
    "        for nucleotide in parent_sequence:\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_sequence += random.choice(NUCLEOTIDES)\n",
    "            else:\n",
    "                mutated_sequence += nucleotide\n",
    "        dataset.append(mutated_sequence)\n",
    "        parent_sequence = mutated_sequence\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_debruijn_graph(dataset: List[str], k: int) -> DefaultDict[str,Set[str]]:\n",
    "    graph = defaultdict(set)\n",
    "    for sequence in dataset:\n",
    "        for i in range(len(sequence) - k):\n",
    "            graph[sequence[i:i + k]].add(sequence[i + 1:i + k + 1])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def extract_substrings(sequences: List[str], sequence_length: int, stride: int, substrings_per_seq: int, protein: bool = False) -> List[str]:\n",
    "    substrings = []\n",
    "    for sequence in sequences:\n",
    "        if not bool(re.match(\"^[ACGT]+$\", sequence)):\n",
    "            continue\n",
    "        if protein:\n",
    "            sequence = dna2protein(sequence)\n",
    "        for i in range(0, len(sequence) - sequence_length + 1, stride):\n",
    "            if i // stride > substrings_per_seq:\n",
    "                break\n",
    "            seq = sequence[i:i + sequence_length]\n",
    "            substrings.append(seq)\n",
    "    return substrings\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, device: torch.device, description: str, print_every: int) -> float:\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    bar = tqdm(train_loader, desc=description)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for i, (inputs, targets) in enumerate(bar):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs, labels=targets)\n",
    "        # loss = outputs.loss.mean()\n",
    "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        if i % print_every == 0:\n",
    "            bar.set_postfix({\"Train Loss\": sum(running_loss) / len(running_loss)})\n",
    "            running_loss = []\n",
    "    return sum(running_loss) / len(running_loss)\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, val_loader: DataLoader, device: torch.device, description: str, config: Config) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    running_loss = []\n",
    "    total_acc = []\n",
    "    bar = tqdm(val_loader, desc=description)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i,(inputs, targets) in enumerate(bar):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, labels=targets)\n",
    "            val_loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            accuracy = (predictions.int() == targets.int()).float().mean().item()\n",
    "            total_loss.append(val_loss.item())\n",
    "            running_loss.append(val_loss.item())\n",
    "            total_acc.append(accuracy)\n",
    "            if (i % config.print_every + 1) == 0:\n",
    "                avg_loss = sum(running_loss) / len(running_loss)\n",
    "                bar.set_postfix({\"Val Loss\": f\"{avg_loss:.5f}\", \"Val Accuracy\": accuracy})\n",
    "                running_loss = []\n",
    "                # bar.set_postfix({\"Val Loss\": val_loss.item(), \"Val Accuracy\": accuracy})\n",
    "    avg_val_loss = sum(total_loss) / len(total_loss)\n",
    "    avg_accuracy = sum(total_acc) / len(total_acc)\n",
    "    return avg_val_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, config: Config) -> None:\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(config.num_epochs):\n",
    "            train_loss = train(model, optimizer, train_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Training\", config.print_every)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Validation\", config)\n",
    "            samples = (epoch+1) * len(train_loader) * config.train_bs\n",
    "            print(f\"Epoch {epoch + 1}/{config.num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Val Accuracy: {val_acc:.5f}\")\n",
    "            wandb.log({\"epoch\": epoch + 1, \"samples\": samples, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "            \n",
    "            if (epoch + 1) % config.save_model_every == 0:\n",
    "                # Save the model weights as an artifact every 10 epochs\n",
    "                artifact = wandb.Artifact(f\"model_weights\", type='model')\n",
    "                torch.save(model, \"gpt2_dna.pt\")\n",
    "                # torch.save(model.state_dict(), 'gpt2_dna.pt')\n",
    "                artifact.add_file('gpt2_dna.pt')\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                num_epochs_no_improve = 0\n",
    "            else:\n",
    "                num_epochs_no_improve += 1\n",
    "                if num_epochs_no_improve >= config.early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}...\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        # torch.save(model.state_dict(), 'gpt2_dna.pt')\n",
    "        torch.save(model, \"gpt2_dna.pt\")\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(config: Config) -> Tuple[List[str], List[str]]:\n",
    "    if config.dataset == DatasetType.HMM:\n",
    "        train_seqs, val_seqs = generate_HMM_dataset(config.sequence_length, N=config.num_seqs, sparsity=config.sparsity,\n",
    "                                                    num_hidden_states=config.num_hidden_states)\n",
    "    elif config.dataset == DatasetType.VIRAL:\n",
    "        set_random_seed(42)\n",
    "        sequences = read_fna(file_path=config.file_path, shuffle=config.sequences_shuffle)\n",
    "        sequences = sequences[:config.num_seqs]\n",
    "        sub_seqs = extract_substrings(sequences, sequence_length=config.sequence_length, stride=config.stride,\n",
    "                                      substrings_per_seq=config.substrings_per_seq, protein=config.protein)\n",
    "        # compute_char_probabilities(sub_seqs)\n",
    "        train_size = int(len(sub_seqs) * (1 - config.split_ratio))\n",
    "        train_seqs = sub_seqs[:train_size]\n",
    "        val_seqs = sub_seqs[train_size:]\n",
    "        train_seqs = train_seqs[:config.num_seqs]\n",
    "        val_seqs = val_seqs[:config.num_seqs]\n",
    "        calculate_entropy(train_seqs, k=2)\n",
    "        calculate_entropy(train_seqs, k=2)\n",
    "    return train_seqs, val_seqs\n",
    "\n",
    "\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = 'cpu'\n",
    "\n",
    "    tokenizer = SequenceTokenizer(protein=config.protein, )\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "    name = f\"{config.dataset.name}_{'protein' if config.protein else 'DNA'}__lr={config.lr}_bs={config.train_bs}_n_embed={config.n_embed}_n_head={config.n_head}_n_layer={config.n_layer}_early_stopping={config.early_stopping_patience}_num_seqs={config.num_seqs}_sequence_length={config.sequence_length}_stride={config.stride}\"\n",
    "    wandb.init(project='GPT2_DNA', name=name, config=config)\n",
    "\n",
    "    gpt2_config = GPT2Config(vocab_size=tokenizer.vocab_size,\n",
    "                             n_positions=config.sequence_length,\n",
    "                             n_ctx=config.sequence_length,\n",
    "                             n_embd=config.n_embed,\n",
    "                             n_layer=config.n_layer,\n",
    "                             n_head=config.n_head)\n",
    "\n",
    "    model = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    train_loop(model, optimizer, train_loader, val_loader, device, config)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model\u001b[39m(config: Config) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     train_seqs, val_seqs \u001b[39m=\u001b[39m load_datasets(config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "def get_model(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = SequenceTokenizer()\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "\n",
    "    model = torch.load('gpt2_dna.pt')\n",
    "\n",
    "    return model, train_loader, val_loader, device\n",
    "\n",
    "model, train_loader, val_loader, device = get_model(Config())\n",
    "\n",
    "inputs, labels = next(iter(val_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "outputs = model(inputs, labels=labels)\n",
    "\n",
    "predicitions = torch.argmax(outputs.logits, dim=-1)\n",
    "tokenizer = SequenceTokenizer()\n",
    "i = 2\n",
    "acc = (predicitions[i] == labels[i]).float().mean().item()\n",
    "print(acc)\n",
    "x = tokenizer.decode(inputs[i])\n",
    "y = tokenizer.decode(predicitions[i])\n",
    "\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AG': 0.060456456456456455, 'GA': 0.06808208208208208, 'GC': 0.06076976976976977, 'CA': 0.06842842842842843, 'AA': 0.08358058058058059, 'AT': 0.0681901901901902, 'TC': 0.0603003003003003, 'CG': 0.057458458458458456, 'GG': 0.05754654654654655, 'GT': 0.054285285285285284, 'TG': 0.06524224224224225, 'CC': 0.05598198198198198, 'TT': 0.07095595595595595, 'TA': 0.0526036036036036, 'AC': 0.06046046046046046, 'CT': 0.05565765765765766}\n",
      "entropy =  1.9947617887039697\n",
      "{'AG': 0.060456456456456455, 'GA': 0.06808208208208208, 'GC': 0.06076976976976977, 'CA': 0.06842842842842843, 'AA': 0.08358058058058059, 'AT': 0.0681901901901902, 'TC': 0.0603003003003003, 'CG': 0.057458458458458456, 'GG': 0.05754654654654655, 'GT': 0.054285285285285284, 'TG': 0.06524224224224225, 'CC': 0.05598198198198198, 'TT': 0.07095595595595595, 'TA': 0.0526036036036036, 'AC': 0.06046046046046046, 'CT': 0.05565765765765766}\n",
      "entropy =  1.9947617887039697\n",
      "Epoch [1/200],  Train Loss: 1.6878, Val Loss: 1.3884\n",
      "Epoch [2/200],  Train Loss: 1.3879, Val Loss: 1.3919\n",
      "Epoch [3/200],  Train Loss: 1.3903, Val Loss: 1.3909\n",
      "Epoch [4/200],  Train Loss: 1.3881, Val Loss: 1.3844\n",
      "Epoch [5/200],  Train Loss: 1.3888, Val Loss: 1.4017\n",
      "Epoch [6/200],  Train Loss: 1.3882, Val Loss: 1.3880\n",
      "Epoch [7/200],  Train Loss: 1.3902, Val Loss: 1.3925\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 1000\n",
    "        self.stride: int = 1000\n",
    "        self.num_seqs: int = 1000\n",
    "        self.split_ratio: float = 0.5\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.protein: bool = False\n",
    "        # self.sparsity: float = 1.1\n",
    "        # self.num_hidden_states: int = 100\n",
    "        self.train_bs: int = 32\n",
    "        self.val_bs: int = 256\n",
    "        self.n_embed: int = 1024\n",
    "        self.n_layer: int = 3\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 0.005\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "\n",
    "# Define the LSTM model\n",
    "class DNAPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(DNAPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.one_hot = lambda x: torch.functional.F.one_hot(x, num_classes=output_size).float()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        input_seq = self.one_hot(input_seq)\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "    \n",
    "\n",
    "config = Config()\n",
    "train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = SequenceTokenizer()\n",
    "train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "\n",
    "output_size = tokenizer.vocab_size\n",
    "# Initialize the model\n",
    "model = DNAPredictor(tokenizer.vocab_size, config.n_embed, tokenizer.vocab_size, num_layers=config.n_layer).to(device)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(config.num_epochs):\n",
    "    total_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1,outputs.shape[-1]), labels.flatten())\n",
    "        total_loss.append(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = sum(total_loss)/len(total_loss)\n",
    "    # print(f'Epoch [{epoch+1}/{config.num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criteria(outputs.view(-1,outputs.shape[-1]), labels.flatten())\n",
    "            total_loss.append(loss.item())\n",
    "        val_loss = sum(total_loss)/len(total_loss)\n",
    "        print(f'Epoch [{epoch+1}/{config.num_epochs}],  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
