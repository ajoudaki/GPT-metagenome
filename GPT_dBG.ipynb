{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xccSsJlVtbf-",
    "outputId": "e7ad6f44-a182-46c6-831a-1ef4c225a05d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirjoudaki\u001b[0m (\u001b[33msketch-bros\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/deepnet-theory/wandb/run-20230613_111603-o22n21xd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/o22n21xd' target=\"_blank\">viruses</a></strong> to <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/o22n21xd' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/o22n21xd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c996547342cb41d9930406fcac496d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7412684549a43a5855e072448d6c26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 1.93486 | Val Loss: 1.44437 | Val Accuracy: 0.24102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ed17c50a7a40e7a77aefd1b778358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24f8c4cc46c4b40a1db8662730c7afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 | Train Loss: 1.42362 | Val Loss: 1.37949 | Val Accuracy: 0.26063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97f8d6501e8407ebdee1002b7bffa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6362fe1464f24b2da3ab6c5e913207f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 | Train Loss: 1.39000 | Val Loss: 1.36998 | Val Accuracy: 0.24952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5b07537ab145199e8f24b722171930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9144c19e174b548e5f55b4f2837709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 | Train Loss: 1.37466 | Val Loss: 1.37012 | Val Accuracy: 0.24569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4955f031ace746d99353723dcdd77ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2b72875cf549a09a144d095f237cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | Train Loss: 1.36605 | Val Loss: 1.36222 | Val Accuracy: 0.28293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704a0f2a8faf4774a3c8210daca0699f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126534bd98d94dfcacf67762ffe1c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 | Train Loss: 1.35723 | Val Loss: 1.35101 | Val Accuracy: 0.26968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e23f7362d54e848ba9c2af196b5205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5d4cdd22e34258966c3f00b9178154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 | Train Loss: 1.34799 | Val Loss: 1.34490 | Val Accuracy: 0.27447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653c059be35d4347a8568c1d5e4f1565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfda8777b7084e4ca632af4ccc08856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 | Train Loss: 1.34484 | Val Loss: 1.34542 | Val Accuracy: 0.27311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e0a0d986c4da1b25acff1897d8050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c05132fb1a349d4a9695daf425c948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 | Train Loss: 1.34265 | Val Loss: 1.34121 | Val Accuracy: 0.27308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82749056cefa465eb03b50fb0adba16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6c9b3667924890bc7f4b3dfe084820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | Train Loss: 1.34078 | Val Loss: 1.33936 | Val Accuracy: 0.27345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635e47d38214703a1c43d75fb71a548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf388317c93847d19af160625eaa6e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/200 | Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 | Train Loss: 1.33837 | Val Loss: 1.33756 | Val Accuracy: 0.27326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c384f342f0b4416a8768c9e6a749fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/200 | Training:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>samples</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_accuracy</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>samples</td><td>11264</td></tr><tr><td>train_loss</td><td>1.33837</td></tr><tr><td>val_accuracy</td><td>0.27326</td></tr><tr><td>val_loss</td><td>1.33756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">viruses</strong> at: <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/o22n21xd' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/o22n21xd</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230613_111603-o22n21xd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, DefaultDict, Set\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "import wandb\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'GPT_dBG.ipynb'\n",
    "\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    HMM = \"HMM\"\n",
    "    VIRAL = \"viral\"\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.HMM\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 400\n",
    "        self.stride: int = 200\n",
    "        self.split_ratio: float = 0.5\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.num_seqs: int = 1000\n",
    "        self.sparsity = 1.1\n",
    "        self.num_hidden_states = 1000\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.train_bs: int = 128\n",
    "        self.val_bs: int = 256\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 6\n",
    "        self.n_head: int = 16\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.print_every: int = 20\n",
    "\n",
    "class SequenceTokenizer:\n",
    "    def __init__(self):\n",
    "        self.alphabet = {'A', 'C', 'G', 'T'}\n",
    "        self.token_to_idx = {char: i for i, char in enumerate(self.alphabet)}\n",
    "        self.idx_to_token = {i: char for i, char in enumerate(self.alphabet)}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "\n",
    "    def encode(self, sequence: str, return_tensors: str = \"pt\") -> torch.Tensor:\n",
    "        tokens = [self.token_to_idx[char] for char in sequence]\n",
    "        if return_tensors == \"pt\":\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        sequence = [self.idx_to_token[token.item()] for token in tokens]\n",
    "        return ''.join(sequence)\n",
    "\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], tokenizer: SequenceTokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence = self.sequences[i]\n",
    "        inputs = self.tokenizer.encode(sequence, return_tensors='pt')\n",
    "        targets = inputs[1:].clone()\n",
    "        inputs = inputs[:-1]\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def compute_char_probabilities(sequences: List[str]) -> None:\n",
    "    counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    for seq in sequences:\n",
    "        cd = Counter(seq)\n",
    "        for c, count in cd.items():\n",
    "            counts[c] += count\n",
    "            total_count += count\n",
    "    pcounts = {char: c / total_count for char, c in counts.items()}\n",
    "    print(pcounts)\n",
    "\n",
    "\n",
    "def read_fna(file_path: str, shuffle: bool = False) -> List[str]:\n",
    "    sequences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"fasta\"):\n",
    "            sequences.append(str(record.seq))\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_markov_chain(n: int, sparsity: float) -> torch.Tensor:\n",
    "    transition_probs = torch.rand(n, n)\n",
    "    for i in range(n):\n",
    "        num_outgoing_states = torch.poisson(torch.tensor([float(sparsity)])).int().item()\n",
    "        num_outgoing_states = min(n, max(1, num_outgoing_states))\n",
    "        _, indices = torch.topk(transition_probs[i], num_outgoing_states)\n",
    "        mask = torch.zeros_like(transition_probs[i]).scatter_(0, indices, 1).to(torch.bool)\n",
    "        transition_probs[i] *= mask\n",
    "    transition_probs = F.normalize(transition_probs, p=1, dim=1)\n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "def draw_seq(transition_probs: torch.Tensor, sequence_length: int) -> str:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    current_state = 0\n",
    "    chain = [current_state]\n",
    "    while len(chain) < sequence_length:\n",
    "        next_state = torch.multinomial(transition_probs[current_state], num_samples=1)\n",
    "        current_state = next_state.item()\n",
    "        chain.append(current_state)\n",
    "    seq = [NUCLEOTIDES[s % len(NUCLEOTIDES)] for s in chain]\n",
    "    return ''.join(seq)\n",
    "\n",
    "\n",
    "def generate_HMM_dataset(sequence_length: int, N: int, sparsity: float, num_hidden_states: int = None) -> Tuple[List[str], List[str]]:\n",
    "    if num_hidden_states is None:\n",
    "        num_hidden_states = sequence_length\n",
    "    transition_matrix = generate_markov_chain(num_hidden_states, sparsity=sparsity)\n",
    "    all_seqs = [[], []]\n",
    "    for _ in range(2):\n",
    "        seqs = []\n",
    "        while len(seqs) < N:\n",
    "            seq = draw_seq(transition_matrix, sequence_length)\n",
    "            if len(seq) >= sequence_length:\n",
    "                seqs.append(seq)\n",
    "        all_seqs[_] = seqs\n",
    "    return all_seqs\n",
    "\n",
    "\n",
    "def generate_phylo_dataset(sequence_length: int, mutation_rate: float, N: int) -> List[str]:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    parent_sequence = ''.join(random.choice(NUCLEOTIDES) for _ in range(sequence_length))\n",
    "    dataset = [parent_sequence]\n",
    "    for _ in range(N):\n",
    "        mutated_sequence = ''\n",
    "        for nucleotide in parent_sequence:\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_sequence += random.choice(NUCLEOTIDES)\n",
    "            else:\n",
    "                mutated_sequence += nucleotide\n",
    "        dataset.append(mutated_sequence)\n",
    "        parent_sequence = mutated_sequence\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_debruijn_graph(dataset: List[str], k: int) -> DefaultDict[str,Set[str]]:\n",
    "    graph = defaultdict(set)\n",
    "    for sequence in dataset:\n",
    "        for i in range(len(sequence) - k):\n",
    "            graph[sequence[i:i + k]].add(sequence[i + 1:i + k + 1])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def extract_substrings(sequences: List[str], sequence_length: int, stride: int, substrings_per_seq: int) -> List[str]:\n",
    "    substrings = []\n",
    "    for sequence in sequences:\n",
    "        for i in range(0, len(sequence) - sequence_length + 1, stride):\n",
    "            if i // stride > substrings_per_seq:\n",
    "                break\n",
    "            seq = sequence[i:i + sequence_length]\n",
    "            if bool(re.match(\"^[ACGT]+$\", seq)):\n",
    "                substrings.append(seq)\n",
    "    return substrings\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, device: torch.device, description: str) -> float:\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    bar = tqdm(train_loader, desc=description)\n",
    "    for inputs, targets in bar:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs, labels=targets)\n",
    "        loss = outputs.loss.mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        bar.set_postfix({\"Train Loss\": loss.item()})\n",
    "    return sum(running_loss) / len(running_loss)\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, val_loader: DataLoader, device: torch.device, description: str) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "    total_acc = 0.0\n",
    "    bar = tqdm(val_loader, desc=description)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, labels=targets)\n",
    "            val_loss = outputs.loss.mean()\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            accuracy = (predictions == targets).cpu().float().mean().item()\n",
    "            total_loss += val_loss.item() * inputs.size(0)\n",
    "            total_count += inputs.size(0)\n",
    "            total_acc += accuracy\n",
    "            bar.set_postfix({\"Val Loss\": val_loss.item(), \"Val Accuracy\": accuracy})\n",
    "    avg_val_loss = total_loss / total_count\n",
    "    avg_accuracy = total_acc / len(val_loader)\n",
    "    return avg_val_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, config: Config) -> None:\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(config.num_epochs):\n",
    "            train_loss = train(model, optimizer, train_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Training\")\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Validation\")\n",
    "            samples = (epoch+1) * len(train_loader) * config.train_bs\n",
    "            print(f\"Epoch {epoch + 1}/{config.num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Val Accuracy: {val_acc:.5f}\")\n",
    "            wandb.log({\"epoch\": epoch + 1, \"samples\": samples, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                # Save the model weights as an artifact every 10 epochs\n",
    "                artifact = wandb.Artifact(f\"model_weights\", type='model')\n",
    "                artifact.add_file('gpt2_dna.pth')\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                num_epochs_no_improve = 0\n",
    "            else:\n",
    "                num_epochs_no_improve += 1\n",
    "                if num_epochs_no_improve >= config.early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}...\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        torch.save(model.state_dict(), 'gpt2_dna.pth')\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(config: Config) -> Tuple[List[str], List[str]]:\n",
    "    if config.dataset == DatasetType.HMM:\n",
    "        train_seqs, val_seqs = generate_HMM_dataset(config.sequence_length, N=config.num_seqs, sparsity=config.sparsity,\n",
    "                                                    num_hidden_states=config.num_hidden_states)\n",
    "    elif config.dataset == DatasetType.VIRAL:\n",
    "        set_random_seed(42)\n",
    "        sequences = read_fna(file_path=config.file_path, shuffle=config.sequences_shuffle)\n",
    "        sequences = sequences[:config.num_seqs]\n",
    "        sub_seqs = extract_substrings(sequences, sequence_length=config.sequence_length, stride=config.stride,\n",
    "                                      substrings_per_seq=config.substrings_per_seq)\n",
    "        compute_char_probabilities(sub_seqs)\n",
    "        train_size = int(len(sub_seqs) * (1 - config.split_ratio))\n",
    "        train_seqs = sub_seqs[:train_size]\n",
    "        val_seqs = sub_seqs[train_size:]\n",
    "        train_seqs = train_seqs[:config.num_seqs]\n",
    "        val_seqs = val_seqs[:config.num_seqs]\n",
    "    return train_seqs, val_seqs\n",
    "\n",
    "\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = SequenceTokenizer()\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_bs, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.val_bs, shuffle=False)\n",
    "\n",
    "    wandb.init(project='GPT2_DNA', name='viruses', config=config)\n",
    "\n",
    "    gpt2_config = GPT2Config(vocab_size=tokenizer.vocab_size,\n",
    "                             n_positions=config.sequence_length,\n",
    "                             n_ctx=config.sequence_length,\n",
    "                             n_embd=config.n_embed,\n",
    "                             n_layer=config.n_layer,\n",
    "                             n_head=config.n_head)\n",
    "\n",
    "    model = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    train_loop(model, optimizer, train_loader, val_loader, device, config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0.27363855788721336, 'G': 0.23881858764807198, 'C': 0.2252636735913389, 'T': 0.26227918087337576}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9i7946by) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105fa14525e944a38a1f904d4f56ded6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>143.8</td></tr><tr><td>train/global_step</td><td>11360</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>1.0913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">viruses</strong> at: <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/9i7946by' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/9i7946by</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230612_170022-9i7946by/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9i7946by). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff68e716d9a4ff3a79aee30329d65c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669005248695613, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/deepnet-theory/wandb/run-20230612_181231-8bvf51l1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/8bvf51l1' target=\"_blank\">viruses</a></strong> to <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/8bvf51l1' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/8bvf51l1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'gradient_accumulation_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     config \u001b[39m=\u001b[39m Config()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     main(config)\n",
      "\u001b[1;32m/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb Cell 3\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mweight_decay)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mnum_epochs,              \u001b[39m# total # of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlogging_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,                         \u001b[39m# the instantiated ü§ó Transformers model to be trained\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     optimizers\u001b[39m=\u001b[39;49m(optimizer, \u001b[39mNone\u001b[39;49;00m),       \u001b[39m# optimizer\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,      \u001b[39m# Modify as needed\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,                          \u001b[39m# if your GPU supports mixed precision\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,         \u001b[39m# training dataset\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mval_dataset,            \u001b[39m# evaluation dataset\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,     \u001b[39m# the function to compute metrics \u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'gradient_accumulation_steps'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 400\n",
    "        self.stride: int = 200\n",
    "        self.split_ratio: float = 0.5\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.num_seqs: int = 10000\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.train_bs: int = 64\n",
    "        self.val_bs: int = 128\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 4\n",
    "        self.n_head: int = 16\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.01\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.warmup_steps: int = 10\n",
    "        self.print_every: int = 20\n",
    "        self.logging_steps: int = 20\n",
    "\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for the DNA dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids: Tensor\n",
    "    labels: Tensor\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], tokenizer: SequenceTokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence = self.sequences[i]\n",
    "        inputs = self.tokenizer.encode(sequence, return_tensors='pt')\n",
    "        targets = inputs[1:].clone()\n",
    "        inputs = inputs[:-1]\n",
    "        return InputExample(input_ids=inputs, labels=targets)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = SequenceTokenizer()\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    wandb.init(project='GPT2_DNA', name='viruses', config=config)\n",
    "\n",
    "    gpt2_config = GPT2Config(vocab_size=tokenizer.vocab_size,\n",
    "                             n_positions=config.sequence_length,\n",
    "                             n_ctx=config.sequence_length,\n",
    "                             n_embd=config.n_embed,\n",
    "                             n_layer=config.n_layer,\n",
    "                             n_head=config.n_head)\n",
    "\n",
    "    model = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=config.num_epochs,              # total # of training epochs\n",
    "        per_device_train_batch_size=config.train_bs,  # batch size per device during training\n",
    "        per_device_eval_batch_size=config.val_bs,   # batch size for evaluation\n",
    "        warmup_steps=config.warmup_steps,                # number of warmup steps for learning rate scheduler\n",
    "        learning_rate=config.lr,         # learning rate\n",
    "        weight_decay=config.weight_decay,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=config.logging_steps,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        optimizers=(optimizer, None),       # optimizer\n",
    "        gradient_accumulation_steps=2,      # Modify as needed\n",
    "        fp16=True,                          # if your GPU supports mixed precision\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the function to compute metrics \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/deepnet-theory/GPT_dBG.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
