{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xccSsJlVtbf-",
    "outputId": "e7ad6f44-a182-46c6-831a-1ef4c225a05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RA': 0.006065065065065065, 'AI': 0.0027672672672672672, 'IG': 0.002278978978978979, 'GA': 0.004077277277277277, 'AP': 0.0036585585585585585, 'PI': 0.0020240240240240242, 'IT': 0.0029721721721721723, 'TR': 0.005290590590590591, 'R*': 0.0043785785785785784, '*I': 0.002543743743743744, 'II': 0.0032987987987987987, 'IK': 0.002590790790790791, 'K*': 0.0018447447447447448, '*L': 0.003723323323323323, 'LT': 0.004916216216216216, 'RN': 0.0029266266266266268, 'NN': 0.0015734734734734735, 'NV': 0.001983983983983984, 'VS': 0.004471671671671671, 'SL': 0.007213113113113113, 'LF': 0.0036776776776776777, 'F*': 0.001904904904904905, '*R': 0.004263963963963964, 'RD': 0.0026816816816816816, 'DR': 0.003032932932932933, 'RI': 0.003704004004004004, 'IF': 0.0025684684684684684, 'FP': 0.0015882882882882882, 'PF': 0.0016730730730730732, 'FF': 0.0020525525525525523, 'FK': 0.00166986986986987, 'KE': 0.0017528528528528528, 'EL': 0.0029997997997997998, 'LK': 0.004083783783783784, 'KQ': 0.0015834834834834834, 'QR': 0.003497997997997998, 'RV': 0.004544044044044044, 'VG': 0.0034072072072072073, 'AC': 0.0016257257257257258, 'CC': 0.0013294294294294293, 'CK': 0.0009866866866866868, 'KN': 0.0020041041041041043, 'NR': 0.0025772772772772773, 'RK': 0.0033787787787787787, 'Q*': 0.0013886886886886887, '*E': 0.001173173173173173, 'EY': 0.0010648648648648648, 'YS': 0.0024006006006006006, 'SS': 0.009875075075075075, 'S*': 0.0037624624624624623, '*K': 0.0023278278278278277, 'KC': 0.0009591591591591592, 'CF': 0.0012171171171171172, 'FS': 0.0033074074074074073, 'SE': 0.0024105105105105104, 'EG': 0.0020837837837837836, 'GY': 0.0016223223223223223, 'Y*': 0.0015224224224224224, '*Y': 0.0014763763763763764, 'YC': 0.000898998998998999, 'CY': 0.0008856856856856857, 'YN': 0.0011382382382382381, 'NE': 0.0012095095095095096, 'EQ': 0.0013892892892892893, 'QS': 0.0025850850850850852, '*D': 0.000903003003003003, 'DT': 0.0016632632632632633, 'TE': 0.001909109109109109, 'EW': 0.000522922922922923, 'WK': 0.0007678678678678678, 'KS': 0.0033552552552552554, 'SV': 0.004629229229229229, 'VY': 0.001591991991991992, 'YF': 0.0014662662662662663, 'FR': 0.002711211211211211, 'RR': 0.013032532532532533, 'RF': 0.002893893893893894, '*C': 0.0014964964964964964, 'FG': 0.0019463463463463464, 'GH': 0.0018193193193193193, 'HL': 0.0027935935935935936, 'FW': 0.0006351351351351352, 'W*': 0.0009127127127127127, 'GN': 0.0019531531531531534, 'RS': 0.009215915915915916, 'SI': 0.004420520520520521, 'RY': 0.002413213213213213, 'IQ': 0.0018262262262262261, 'QA': 0.0022227227227227227, 'AS': 0.00576926926926927, 'SY': 0.002238038038038038, '*A': 0.0018424424424424424, 'AE': 0.0024654654654654657, 'ER': 0.0030703703703703704, 'QL': 0.0031633633633633635, 'LE': 0.002915815815815816, 'EK': 0.0016515515515515516, '*F': 0.0017057057057057057, 'FT': 0.002047647647647648, 'TC': 0.0016621621621621622, 'CW': 0.0007182182182182182, 'KW': 0.0006542542542542542, '**': 0.00277007007007007, '*G': 0.0017952952952952953, 'GE': 0.0021901901901901903, 'EF': 0.0011842842842842843, 'FH': 0.0011477477477477478, 'HH': 0.0014098098098098099, 'HK': 0.0008862862862862863, 'YA': 0.001402902902902903, 'AN': 0.0018595595595595595, 'NM': 0.0006134134134134135, 'ME': 0.000655055055055055, 'A*': 0.0019514514514514514, 'IH': 0.0014294294294294294, 'HA': 0.001605005005005005, 'AH': 0.001570870870870871, 'HI': 0.0012403403403403403, 'KT': 0.0026226226226226225, 'TH': 0.0013882882882882883, 'HN': 0.0008277277277277277, 'NL': 0.0032485485485485486, 'LG': 0.004318718718718719, 'GC': 0.0018857857857857857, 'CP': 0.0014811811811811812, 'PY': 0.0011642642642642643, 'NP': 0.001680980980980981, 'P*': 0.0019213213213213213, '*H': 0.0011985985985985986, 'H*': 0.0012876876876876876, 'QC': 0.000793093093093093, 'C*': 0.0016726726726726726, '*N': 0.0018707707707707707, 'NY': 0.0012143143143143144, 'FD': 0.0013175175175175176, 'D*': 0.0010476476476476476, '*P': 0.0022319319319319317, 'PK': 0.0017253253253253253, 'QN': 0.001305005005005005, 'ND': 0.0011947947947947948, 'DQ': 0.0012313313313313313, 'RE': 0.0026571571571571573, 'EN': 0.0013333333333333333, 'NT': 0.002060760760760761, 'QT': 0.0019645645645645646, 'TM': 0.0010244244244244243, 'ML': 0.0016635635635635635, 'LI': 0.004691691691691691, 'KR': 0.003382882882882883, 'YV': 0.0014671671671671672, 'VF': 0.0022734734734734733, 'FY': 0.0013855855855855856, 'YL': 0.0028183183183183185, 'LP': 0.004957757757757758, 'PS': 0.005194194194194194, 'LV': 0.005211811811811812, 'VI': 0.00282992992992993, 'IP': 0.0023026026026026027, 'PE': 0.002055055055055055, 'KK': 0.0026043043043043044, 'KH': 0.0011305305305305306, 'HS': 0.0021637637637637636, 'SW': 0.0016415415415415415, 'WA': 0.0008834834834834835, 'AV': 0.003998698698698699, 'VH': 0.001708908908908909, 'HE': 0.0009265265265265265, 'EA': 0.0023266266266266265, '*S': 0.003887187187187187, 'SD': 0.0024740740740740743, 'EI': 0.001835035035035035, 'IC': 0.001432032032032032, 'LC': 0.0022926926926926925, 'CL': 0.002424324324324324, 'CH': 0.0008325325325325325, 'HV': 0.001512012012012012, 'FL': 0.003954454454454455, 'EE': 0.0017341341341341342, 'LL': 0.00943053053053053, 'LR': 0.007345945945945946, 'CI': 0.0013412412412412411, 'IR': 0.0035752752752752754, 'L*': 0.003940940940940941, 'CA': 0.001635935935935936, 'AY': 0.001401901901901902, 'YQ': 0.0012218218218218219, 'IN': 0.0021904904904904905, 'SM': 0.0014797797797797799, 'MQ': 0.0005964964964964965, 'QV': 0.0020240240240240242, 'VN': 0.0019362362362362363, 'T*': 0.0022365365365365365, 'N*': 0.0016179179179179179, '*W': 0.0008092092092092092, 'WE': 0.00044424424424424424, 'ES': 0.0023725725725725726, 'SN': 0.0029427427427427427, 'NS': 0.0029133133133133135, 'FA': 0.0018787787787787789, 'SC': 0.002741141141141141, 'CR': 0.0030931931931931934, 'KA': 0.002294794794794795, 'SF': 0.003398998998998999, 'RT': 0.005648048048048048, 'YR': 0.002385085085085085, 'DG': 0.0025665665665665664, 'G*': 0.002341741741741742, 'RL': 0.0074517517517517515, 'IL': 0.00483973973973974, 'LS': 0.007034234234234234, 'LN': 0.0032106106106106107, 'YI': 0.0016934934934934936, 'IY': 0.0018098098098098098, 'QP': 0.0020663663663663665, 'FE': 0.0011978978978978978, 'E*': 0.0010756756756756757, 'KF': 0.0015512512512512512, 'IM': 0.0009269269269269269, 'MI': 0.001016016016016016, 'VC': 0.0015287287287287287, 'CV': 0.001405005005005005, 'FV': 0.0021435435435435434, 'IV': 0.0026973973973973974, 'VL': 0.005374474474474474, 'FI': 0.0024645645645645646, 'NK': 0.0017016016016016015, 'KI': 0.0026272272272272273, 'IS': 0.004416516516516517, 'SK': 0.003360660660660661, '*T': 0.0026833833833833833, 'TK': 0.002461861861861862, 'QK': 0.00164004004004004, 'TI': 0.003010810810810811, 'I*': 0.0025488488488488487, 'NF': 0.0015528528528528528, 'GF': 0.002036936936936937, 'SA': 0.00612062062062062, 'AF': 0.00197977977977978, 'WI': 0.0008419419419419419, 'TF': 0.002135135135135135, 'KY': 0.0014206206206206207, 'NH': 0.0011064064064064065, 'GI': 0.0025781781781781784, 'PD': 0.0020716716716716716, 'DL': 0.0028549549549549548, 'CN': 0.0009177177177177177, 'EH': 0.0010406406406406406, 'LW': 0.0013247247247247247, 'QW': 0.0005196196196196196, 'WH': 0.00047547547547547545, 'LH': 0.0027417417417417416, 'MS': 0.0013234234234234235, 'YT': 0.001467967967967968, 'YM': 0.00044024024024024024, 'PR': 0.006042542542542543, 'LD': 0.0028416416416416415, 'KL': 0.003798898898898899, 'PN': 0.0014473473473473474, 'VP': 0.0029474474474474473, 'SP': 0.005155855855855856, 'NW': 0.0005914914914914915, '*M': 0.0007907907907907908, 'MY': 0.00047857857857857856, 'DV': 0.002108408408408408, 'HF': 0.0010403403403403404, 'YH': 0.0009506506506506507, 'HQ': 0.0014942942942942942, 'LY': 0.0027163163163163163, 'LQ': 0.0033083083083083084, 'HP': 0.0018416416416416417, 'PP': 0.003625125125125125, 'EC': 0.0007124124124124124, 'PL': 0.004163263263263263, 'DP': 0.0017748748748748749, 'GL': 0.004660260260260261, 'TS': 0.006017417417417417, 'KP': 0.001955955955955956, 'EM': 0.0006182182182182183, 'MF': 0.0006364364364364365, 'ID': 0.001712012012012012, 'QI': 0.001768968968968969, 'YP': 0.001267967967967968, 'LM': 0.0018334334334334334, 'MK': 0.0009634634634634635, 'KM': 0.0009145145145145145, 'M*': 0.0006061061061061061, 'NQ': 0.0013675675675675675, 'QF': 0.0011546546546546547, 'QH': 0.0012393393393393393, 'IE': 0.0017336336336336337, 'PQ': 0.0016736736736736736, 'AL': 0.0048900900900900905, 'GS': 0.005129729729729729, 'PW': 0.0008783783783783784, 'PC': 0.0015305305305305305, 'CT': 0.0015273273273273273, 'TV': 0.0033184184184184185, 'YY': 0.0011346346346346346, 'SH': 0.0019913913913913914, 'WM': 0.00033503503503503506, 'RW': 0.0019146146146146147, 'HC': 0.0008312312312312312, 'CM': 0.0003633633633633634, 'MC': 0.0003373373373373373, 'IA': 0.0025923923923923922, 'QQ': 0.0016730730730730732, 'HY': 0.0008514514514514514, 'SR': 0.008293993993993994, 'ET': 0.0018484484484484484, 'VE': 0.002193193193193193, 'ED': 0.0014558558558558558, 'VQ': 0.002042042042042042, 'SQ': 0.0024334334334334333, 'V*': 0.002169169169169169, 'CS': 0.0030394394394394394, 'RC': 0.003325125125125125, 'CE': 0.0006928928928928929, 'WT': 0.0010319319319319318, 'VD': 0.002244744744744745, 'DS': 0.002358958958958959, 'FC': 0.0012057057057057057, 'HG': 0.0019114114114114115, 'NA': 0.0019541541541541543, 'HR': 0.003376676676676677, 'DN': 0.0011196196196196196, 'PT': 0.003544744744744745, '*V': 0.0020075075075075073, 'CQ': 0.0009134134134134134, 'WN': 0.000632932932932933, 'NI': 0.0022352352352352353, 'TP': 0.0036803803803803804, 'RH': 0.003033033033033033, 'PH': 0.001474074074074074, 'RG': 0.005657457457457458, 'LA': 0.005048948948948949, 'QE': 0.0013872872872872873, 'EV': 0.0021883883883883886, 'RP': 0.006271971971971972, 'GW': 0.0011067067067067067, 'WR': 0.0019226226226226226, 'GG': 0.004273473473473474, 'WL': 0.001605905905905906, 'QG': 0.0020332332332332332, 'GP': 0.0028178178178178176, 'AQ': 0.0020061061061061062, 'PM': 0.0007661661661661662, 'MG': 0.0008086086086086087, 'VK': 0.0023186186186186187, 'SG': 0.005625425425425426, 'AR': 0.006236436436436436, 'QM': 0.0006414414414414414, 'MN': 0.000688088088088088, 'QD': 0.001164064064064064, 'DF': 0.0012762762762762762, 'YK': 0.0012463463463463463, 'TY': 0.0014943943943943943, 'MW': 0.00022912912912912914, 'AD': 0.0026025025025025023, 'HD': 0.001023823823823824, 'WC': 0.0006842842842842843, 'NC': 0.0009689689689689689, 'CD': 0.000736936936936937, 'PG': 0.0039036036036036036, 'HT': 0.0013216216216216216, 'TD': 0.00181011011011011, 'GD': 0.0023462462462462462, 'DA': 0.0024034034034034033, 'GM': 0.00087997997997998, 'MA': 0.0011752752752752754, 'AA': 0.005987087087087087, 'VA': 0.003681781781781782, 'GR': 0.006606806806806807, 'TL': 0.0047732732732732736, 'AM': 0.0010573573573573573, 'MT': 0.0010953953953953953, 'YG': 0.0015619619619619619, 'KV': 0.00232012012012012, 'FN': 0.0015806806806806808, 'TQ': 0.0017383383383383383, 'NG': 0.0021761761761761762, 'GQ': 0.0020844844844844844, 'TN': 0.0020863863863863864, 'KD': 0.0013367367367367368, 'TT': 0.0044998998998999, 'AK': 0.0021802802802802805, 'DI': 0.001624024024024024, 'VM': 0.0010193193193193193, 'TA': 0.004256756756756757, 'GV': 0.003584984984984985, 'HM': 0.00034594594594594593, 'VT': 0.003085985985985986, 'ST': 0.0061615615615615614, 'MM': 0.000497997997997998, 'AT': 0.003997497497497497, 'TG': 0.0037114114114114114, 'GT': 0.003345945945945946, 'CG': 0.0018533533533533534, 'VV': 0.003897297297297297, 'DY': 0.0009895895895895895, 'TW': 0.000994994994994995, 'WF': 0.0006497497497497497, 'RQ': 0.0032697697697697697, 'PA': 0.0046735735735735735, 'MR': 0.0012121121121121121, 'DH': 0.001081081081081081, 'KG': 0.0019134134134134134, 'FQ': 0.001371971971971972, 'DK': 0.0012674674674674675, 'WS': 0.0015406406406406406, 'DE': 0.0015923923923923924, 'YD': 0.0009641641641641641, 'PV': 0.0033586586586586585, 'GK': 0.0022166166166166165, 'MD': 0.0005801801801801801, 'DC': 0.0008235235235235235, 'DD': 0.0017017017017017016, 'MP': 0.0008473473473473473, 'FM': 0.0006122122122122122, 'YE': 0.0009425425425425426, 'QY': 0.0009997997997997997, 'WV': 0.0008306306306306307, 'MH': 0.00035645645645645646, 'RM': 0.001293093093093093, 'IW': 0.0008141141141141141, 'WD': 0.0004131131131131131, 'DM': 0.0005166166166166166, 'EP': 0.0017321321321321322, 'VR': 0.004813813813813814, 'AG': 0.005044244244244244, 'MV': 0.0012091091091091092, 'WQ': 0.0006083083083083084, 'DW': 0.0005411411411411412, 'AW': 0.0010807807807807807, 'WY': 0.0005122122122122122, 'WP': 0.001015015015015015, 'YW': 0.00047417417417417417, 'VW': 0.0009151151151151151, 'HW': 0.00043263263263263266, 'WW': 0.00044854854854854855, 'WG': 0.000688988988988989, '*Q': 0.0013986986986986986, 'KX': 1.001001001001001e-07}\n",
      "entropy =  4.2376711423858096\n",
      "{'RA': 0.006065065065065065, 'AI': 0.0027672672672672672, 'IG': 0.002278978978978979, 'GA': 0.004077277277277277, 'AP': 0.0036585585585585585, 'PI': 0.0020240240240240242, 'IT': 0.0029721721721721723, 'TR': 0.005290590590590591, 'R*': 0.0043785785785785784, '*I': 0.002543743743743744, 'II': 0.0032987987987987987, 'IK': 0.002590790790790791, 'K*': 0.0018447447447447448, '*L': 0.003723323323323323, 'LT': 0.004916216216216216, 'RN': 0.0029266266266266268, 'NN': 0.0015734734734734735, 'NV': 0.001983983983983984, 'VS': 0.004471671671671671, 'SL': 0.007213113113113113, 'LF': 0.0036776776776776777, 'F*': 0.001904904904904905, '*R': 0.004263963963963964, 'RD': 0.0026816816816816816, 'DR': 0.003032932932932933, 'RI': 0.003704004004004004, 'IF': 0.0025684684684684684, 'FP': 0.0015882882882882882, 'PF': 0.0016730730730730732, 'FF': 0.0020525525525525523, 'FK': 0.00166986986986987, 'KE': 0.0017528528528528528, 'EL': 0.0029997997997997998, 'LK': 0.004083783783783784, 'KQ': 0.0015834834834834834, 'QR': 0.003497997997997998, 'RV': 0.004544044044044044, 'VG': 0.0034072072072072073, 'AC': 0.0016257257257257258, 'CC': 0.0013294294294294293, 'CK': 0.0009866866866866868, 'KN': 0.0020041041041041043, 'NR': 0.0025772772772772773, 'RK': 0.0033787787787787787, 'Q*': 0.0013886886886886887, '*E': 0.001173173173173173, 'EY': 0.0010648648648648648, 'YS': 0.0024006006006006006, 'SS': 0.009875075075075075, 'S*': 0.0037624624624624623, '*K': 0.0023278278278278277, 'KC': 0.0009591591591591592, 'CF': 0.0012171171171171172, 'FS': 0.0033074074074074073, 'SE': 0.0024105105105105104, 'EG': 0.0020837837837837836, 'GY': 0.0016223223223223223, 'Y*': 0.0015224224224224224, '*Y': 0.0014763763763763764, 'YC': 0.000898998998998999, 'CY': 0.0008856856856856857, 'YN': 0.0011382382382382381, 'NE': 0.0012095095095095096, 'EQ': 0.0013892892892892893, 'QS': 0.0025850850850850852, '*D': 0.000903003003003003, 'DT': 0.0016632632632632633, 'TE': 0.001909109109109109, 'EW': 0.000522922922922923, 'WK': 0.0007678678678678678, 'KS': 0.0033552552552552554, 'SV': 0.004629229229229229, 'VY': 0.001591991991991992, 'YF': 0.0014662662662662663, 'FR': 0.002711211211211211, 'RR': 0.013032532532532533, 'RF': 0.002893893893893894, '*C': 0.0014964964964964964, 'FG': 0.0019463463463463464, 'GH': 0.0018193193193193193, 'HL': 0.0027935935935935936, 'FW': 0.0006351351351351352, 'W*': 0.0009127127127127127, 'GN': 0.0019531531531531534, 'RS': 0.009215915915915916, 'SI': 0.004420520520520521, 'RY': 0.002413213213213213, 'IQ': 0.0018262262262262261, 'QA': 0.0022227227227227227, 'AS': 0.00576926926926927, 'SY': 0.002238038038038038, '*A': 0.0018424424424424424, 'AE': 0.0024654654654654657, 'ER': 0.0030703703703703704, 'QL': 0.0031633633633633635, 'LE': 0.002915815815815816, 'EK': 0.0016515515515515516, '*F': 0.0017057057057057057, 'FT': 0.002047647647647648, 'TC': 0.0016621621621621622, 'CW': 0.0007182182182182182, 'KW': 0.0006542542542542542, '**': 0.00277007007007007, '*G': 0.0017952952952952953, 'GE': 0.0021901901901901903, 'EF': 0.0011842842842842843, 'FH': 0.0011477477477477478, 'HH': 0.0014098098098098099, 'HK': 0.0008862862862862863, 'YA': 0.001402902902902903, 'AN': 0.0018595595595595595, 'NM': 0.0006134134134134135, 'ME': 0.000655055055055055, 'A*': 0.0019514514514514514, 'IH': 0.0014294294294294294, 'HA': 0.001605005005005005, 'AH': 0.001570870870870871, 'HI': 0.0012403403403403403, 'KT': 0.0026226226226226225, 'TH': 0.0013882882882882883, 'HN': 0.0008277277277277277, 'NL': 0.0032485485485485486, 'LG': 0.004318718718718719, 'GC': 0.0018857857857857857, 'CP': 0.0014811811811811812, 'PY': 0.0011642642642642643, 'NP': 0.001680980980980981, 'P*': 0.0019213213213213213, '*H': 0.0011985985985985986, 'H*': 0.0012876876876876876, 'QC': 0.000793093093093093, 'C*': 0.0016726726726726726, '*N': 0.0018707707707707707, 'NY': 0.0012143143143143144, 'FD': 0.0013175175175175176, 'D*': 0.0010476476476476476, '*P': 0.0022319319319319317, 'PK': 0.0017253253253253253, 'QN': 0.001305005005005005, 'ND': 0.0011947947947947948, 'DQ': 0.0012313313313313313, 'RE': 0.0026571571571571573, 'EN': 0.0013333333333333333, 'NT': 0.002060760760760761, 'QT': 0.0019645645645645646, 'TM': 0.0010244244244244243, 'ML': 0.0016635635635635635, 'LI': 0.004691691691691691, 'KR': 0.003382882882882883, 'YV': 0.0014671671671671672, 'VF': 0.0022734734734734733, 'FY': 0.0013855855855855856, 'YL': 0.0028183183183183185, 'LP': 0.004957757757757758, 'PS': 0.005194194194194194, 'LV': 0.005211811811811812, 'VI': 0.00282992992992993, 'IP': 0.0023026026026026027, 'PE': 0.002055055055055055, 'KK': 0.0026043043043043044, 'KH': 0.0011305305305305306, 'HS': 0.0021637637637637636, 'SW': 0.0016415415415415415, 'WA': 0.0008834834834834835, 'AV': 0.003998698698698699, 'VH': 0.001708908908908909, 'HE': 0.0009265265265265265, 'EA': 0.0023266266266266265, '*S': 0.003887187187187187, 'SD': 0.0024740740740740743, 'EI': 0.001835035035035035, 'IC': 0.001432032032032032, 'LC': 0.0022926926926926925, 'CL': 0.002424324324324324, 'CH': 0.0008325325325325325, 'HV': 0.001512012012012012, 'FL': 0.003954454454454455, 'EE': 0.0017341341341341342, 'LL': 0.00943053053053053, 'LR': 0.007345945945945946, 'CI': 0.0013412412412412411, 'IR': 0.0035752752752752754, 'L*': 0.003940940940940941, 'CA': 0.001635935935935936, 'AY': 0.001401901901901902, 'YQ': 0.0012218218218218219, 'IN': 0.0021904904904904905, 'SM': 0.0014797797797797799, 'MQ': 0.0005964964964964965, 'QV': 0.0020240240240240242, 'VN': 0.0019362362362362363, 'T*': 0.0022365365365365365, 'N*': 0.0016179179179179179, '*W': 0.0008092092092092092, 'WE': 0.00044424424424424424, 'ES': 0.0023725725725725726, 'SN': 0.0029427427427427427, 'NS': 0.0029133133133133135, 'FA': 0.0018787787787787789, 'SC': 0.002741141141141141, 'CR': 0.0030931931931931934, 'KA': 0.002294794794794795, 'SF': 0.003398998998998999, 'RT': 0.005648048048048048, 'YR': 0.002385085085085085, 'DG': 0.0025665665665665664, 'G*': 0.002341741741741742, 'RL': 0.0074517517517517515, 'IL': 0.00483973973973974, 'LS': 0.007034234234234234, 'LN': 0.0032106106106106107, 'YI': 0.0016934934934934936, 'IY': 0.0018098098098098098, 'QP': 0.0020663663663663665, 'FE': 0.0011978978978978978, 'E*': 0.0010756756756756757, 'KF': 0.0015512512512512512, 'IM': 0.0009269269269269269, 'MI': 0.001016016016016016, 'VC': 0.0015287287287287287, 'CV': 0.001405005005005005, 'FV': 0.0021435435435435434, 'IV': 0.0026973973973973974, 'VL': 0.005374474474474474, 'FI': 0.0024645645645645646, 'NK': 0.0017016016016016015, 'KI': 0.0026272272272272273, 'IS': 0.004416516516516517, 'SK': 0.003360660660660661, '*T': 0.0026833833833833833, 'TK': 0.002461861861861862, 'QK': 0.00164004004004004, 'TI': 0.003010810810810811, 'I*': 0.0025488488488488487, 'NF': 0.0015528528528528528, 'GF': 0.002036936936936937, 'SA': 0.00612062062062062, 'AF': 0.00197977977977978, 'WI': 0.0008419419419419419, 'TF': 0.002135135135135135, 'KY': 0.0014206206206206207, 'NH': 0.0011064064064064065, 'GI': 0.0025781781781781784, 'PD': 0.0020716716716716716, 'DL': 0.0028549549549549548, 'CN': 0.0009177177177177177, 'EH': 0.0010406406406406406, 'LW': 0.0013247247247247247, 'QW': 0.0005196196196196196, 'WH': 0.00047547547547547545, 'LH': 0.0027417417417417416, 'MS': 0.0013234234234234235, 'YT': 0.001467967967967968, 'YM': 0.00044024024024024024, 'PR': 0.006042542542542543, 'LD': 0.0028416416416416415, 'KL': 0.003798898898898899, 'PN': 0.0014473473473473474, 'VP': 0.0029474474474474473, 'SP': 0.005155855855855856, 'NW': 0.0005914914914914915, '*M': 0.0007907907907907908, 'MY': 0.00047857857857857856, 'DV': 0.002108408408408408, 'HF': 0.0010403403403403404, 'YH': 0.0009506506506506507, 'HQ': 0.0014942942942942942, 'LY': 0.0027163163163163163, 'LQ': 0.0033083083083083084, 'HP': 0.0018416416416416417, 'PP': 0.003625125125125125, 'EC': 0.0007124124124124124, 'PL': 0.004163263263263263, 'DP': 0.0017748748748748749, 'GL': 0.004660260260260261, 'TS': 0.006017417417417417, 'KP': 0.001955955955955956, 'EM': 0.0006182182182182183, 'MF': 0.0006364364364364365, 'ID': 0.001712012012012012, 'QI': 0.001768968968968969, 'YP': 0.001267967967967968, 'LM': 0.0018334334334334334, 'MK': 0.0009634634634634635, 'KM': 0.0009145145145145145, 'M*': 0.0006061061061061061, 'NQ': 0.0013675675675675675, 'QF': 0.0011546546546546547, 'QH': 0.0012393393393393393, 'IE': 0.0017336336336336337, 'PQ': 0.0016736736736736736, 'AL': 0.0048900900900900905, 'GS': 0.005129729729729729, 'PW': 0.0008783783783783784, 'PC': 0.0015305305305305305, 'CT': 0.0015273273273273273, 'TV': 0.0033184184184184185, 'YY': 0.0011346346346346346, 'SH': 0.0019913913913913914, 'WM': 0.00033503503503503506, 'RW': 0.0019146146146146147, 'HC': 0.0008312312312312312, 'CM': 0.0003633633633633634, 'MC': 0.0003373373373373373, 'IA': 0.0025923923923923922, 'QQ': 0.0016730730730730732, 'HY': 0.0008514514514514514, 'SR': 0.008293993993993994, 'ET': 0.0018484484484484484, 'VE': 0.002193193193193193, 'ED': 0.0014558558558558558, 'VQ': 0.002042042042042042, 'SQ': 0.0024334334334334333, 'V*': 0.002169169169169169, 'CS': 0.0030394394394394394, 'RC': 0.003325125125125125, 'CE': 0.0006928928928928929, 'WT': 0.0010319319319319318, 'VD': 0.002244744744744745, 'DS': 0.002358958958958959, 'FC': 0.0012057057057057057, 'HG': 0.0019114114114114115, 'NA': 0.0019541541541541543, 'HR': 0.003376676676676677, 'DN': 0.0011196196196196196, 'PT': 0.003544744744744745, '*V': 0.0020075075075075073, 'CQ': 0.0009134134134134134, 'WN': 0.000632932932932933, 'NI': 0.0022352352352352353, 'TP': 0.0036803803803803804, 'RH': 0.003033033033033033, 'PH': 0.001474074074074074, 'RG': 0.005657457457457458, 'LA': 0.005048948948948949, 'QE': 0.0013872872872872873, 'EV': 0.0021883883883883886, 'RP': 0.006271971971971972, 'GW': 0.0011067067067067067, 'WR': 0.0019226226226226226, 'GG': 0.004273473473473474, 'WL': 0.001605905905905906, 'QG': 0.0020332332332332332, 'GP': 0.0028178178178178176, 'AQ': 0.0020061061061061062, 'PM': 0.0007661661661661662, 'MG': 0.0008086086086086087, 'VK': 0.0023186186186186187, 'SG': 0.005625425425425426, 'AR': 0.006236436436436436, 'QM': 0.0006414414414414414, 'MN': 0.000688088088088088, 'QD': 0.001164064064064064, 'DF': 0.0012762762762762762, 'YK': 0.0012463463463463463, 'TY': 0.0014943943943943943, 'MW': 0.00022912912912912914, 'AD': 0.0026025025025025023, 'HD': 0.001023823823823824, 'WC': 0.0006842842842842843, 'NC': 0.0009689689689689689, 'CD': 0.000736936936936937, 'PG': 0.0039036036036036036, 'HT': 0.0013216216216216216, 'TD': 0.00181011011011011, 'GD': 0.0023462462462462462, 'DA': 0.0024034034034034033, 'GM': 0.00087997997997998, 'MA': 0.0011752752752752754, 'AA': 0.005987087087087087, 'VA': 0.003681781781781782, 'GR': 0.006606806806806807, 'TL': 0.0047732732732732736, 'AM': 0.0010573573573573573, 'MT': 0.0010953953953953953, 'YG': 0.0015619619619619619, 'KV': 0.00232012012012012, 'FN': 0.0015806806806806808, 'TQ': 0.0017383383383383383, 'NG': 0.0021761761761761762, 'GQ': 0.0020844844844844844, 'TN': 0.0020863863863863864, 'KD': 0.0013367367367367368, 'TT': 0.0044998998998999, 'AK': 0.0021802802802802805, 'DI': 0.001624024024024024, 'VM': 0.0010193193193193193, 'TA': 0.004256756756756757, 'GV': 0.003584984984984985, 'HM': 0.00034594594594594593, 'VT': 0.003085985985985986, 'ST': 0.0061615615615615614, 'MM': 0.000497997997997998, 'AT': 0.003997497497497497, 'TG': 0.0037114114114114114, 'GT': 0.003345945945945946, 'CG': 0.0018533533533533534, 'VV': 0.003897297297297297, 'DY': 0.0009895895895895895, 'TW': 0.000994994994994995, 'WF': 0.0006497497497497497, 'RQ': 0.0032697697697697697, 'PA': 0.0046735735735735735, 'MR': 0.0012121121121121121, 'DH': 0.001081081081081081, 'KG': 0.0019134134134134134, 'FQ': 0.001371971971971972, 'DK': 0.0012674674674674675, 'WS': 0.0015406406406406406, 'DE': 0.0015923923923923924, 'YD': 0.0009641641641641641, 'PV': 0.0033586586586586585, 'GK': 0.0022166166166166165, 'MD': 0.0005801801801801801, 'DC': 0.0008235235235235235, 'DD': 0.0017017017017017016, 'MP': 0.0008473473473473473, 'FM': 0.0006122122122122122, 'YE': 0.0009425425425425426, 'QY': 0.0009997997997997997, 'WV': 0.0008306306306306307, 'MH': 0.00035645645645645646, 'RM': 0.001293093093093093, 'IW': 0.0008141141141141141, 'WD': 0.0004131131131131131, 'DM': 0.0005166166166166166, 'EP': 0.0017321321321321322, 'VR': 0.004813813813813814, 'AG': 0.005044244244244244, 'MV': 0.0012091091091091092, 'WQ': 0.0006083083083083084, 'DW': 0.0005411411411411412, 'AW': 0.0010807807807807807, 'WY': 0.0005122122122122122, 'WP': 0.001015015015015015, 'YW': 0.00047417417417417417, 'VW': 0.0009151151151151151, 'HW': 0.00043263263263263266, 'WW': 0.00044854854854854855, 'WG': 0.000688988988988989, '*Q': 0.0013986986986986986, 'KX': 1.001001001001001e-07}\n",
      "entropy =  4.2376711423858096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirjoudaki\u001b[0m (\u001b[33msketch-bros\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/GPT_metagenome/wandb/run-20230614_174422-7zdqxnep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/7zdqxnep' target=\"_blank\">VIRAL_protein__lr=0.0001_bs=16_n_embed=512_n_head=1_n_layer=6_early_stopping=5_num_seqs=10000_sequence_length=1000_stride=1000</a></strong> to <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketch-bros/GPT2_DNA' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketch-bros/GPT2_DNA/runs/7zdqxnep' target=\"_blank\">https://wandb.ai/sketch-bros/GPT2_DNA/runs/7zdqxnep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30db37547824898a167f3a8410dfcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200 | Training:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c9fa22fdf94d5885b27f88b09e1626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200 | Validation:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 2.85786 | Val Loss: 2.84106 | Val Accuracy: 0.12670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa10ff151af74fb5ace16aed01cfce87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200 | Training:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286f3cab5b884133b3b2fc758d5d055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200 | Validation:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 | Train Loss: 2.80991 | Val Loss: 2.82742 | Val Accuracy: 0.13002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cc0e03728342db9d07903be5aaf6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200 | Training:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0df67d594d44eef9d6663a2b79f8b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200 | Validation:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 | Train Loss: 2.84227 | Val Loss: 2.82490 | Val Accuracy: 0.13098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c9ee03386449e08accea9f03c5fb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200 | Training:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82603b58c5745519a037d8fef174843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200 | Validation:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 | Train Loss: 2.81720 | Val Loss: 2.82303 | Val Accuracy: 0.13084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31aa25b810e94d10b8f739292977c9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200 | Training:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbdb10a88854855ab80a36b967f80cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200 | Validation:   0%|          | 0/312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, DefaultDict, Set\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "import wandb\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'GPT_dBG.ipynb'\n",
    "\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    HMM = \"HMM\"\n",
    "    VIRAL = \"viral\"\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.HMM\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.sequence_length: int = 1000\n",
    "        self.num_seqs: int = 500\n",
    "        self.num_hidden_states: int = 500\n",
    "        self.sparsity: float = 1.1\n",
    "        self.protein: bool = True\n",
    "        # self.stride: int = 1000\n",
    "        # self.split_ratio: float = 0.5\n",
    "        # self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        # self.substrings_per_seq: int = 20\n",
    "        # self.sequences_shuffle: bool = True\n",
    "        self.train_bs: int = 64\n",
    "        self.val_bs: int = 128\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 4\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 3\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 1000\n",
    "        self.stride: int = 1000\n",
    "        self.num_seqs: int = 100000\n",
    "        self.split_ratio: float = 0.2\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.protein: bool = True\n",
    "        self.sparsity: float = 1.1\n",
    "        self.num_hidden_states: int = 100\n",
    "        self.train_bs: int = 16\n",
    "        self.val_bs: int = 32\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 6\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "        \n",
    "# DNA codon table\n",
    "dna_codon_table = {\n",
    "    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L',\n",
    "    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S',\n",
    "    'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*',\n",
    "    'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W',\n",
    "    'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n",
    "    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
    "    'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n",
    "    'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n",
    "    'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M',\n",
    "    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
    "    'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n",
    "    'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n",
    "    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n",
    "    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
    "    'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n",
    "    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'\n",
    "}\n",
    "\n",
    "\n",
    "def dna2protein(dna_sequence):\n",
    "    protein_dna = ''.join(dna_codon_table.get(dna_sequence[i:i+3], 'X') for i in range(0, len(dna_sequence), 3))\n",
    "    return protein_dna\n",
    "\n",
    "class SequenceTokenizer:\n",
    "    def __init__(self, protein: bool = False):\n",
    "        if protein:\n",
    "            self.alphabet = list(dna_codon_table.values())+['X']\n",
    "            self.alphabet = set(self.alphabet)\n",
    "        else:\n",
    "            self.alphabet = {'A', 'C', 'G', 'T', 'X'}\n",
    "        self.token_to_idx = {char: i for i, char in enumerate(self.alphabet)}\n",
    "        self.idx_to_token = {i: char for i, char in enumerate(self.alphabet)}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "\n",
    "    def encode(self, sequence: str, return_tensors: str = \"pt\") -> torch.Tensor:\n",
    "        tokens = [self.token_to_idx[char] for char in sequence]\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        sequence = [self.idx_to_token[int(token.item())] for token in tokens]\n",
    "        return ''.join(sequence)\n",
    "\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], tokenizer: SequenceTokenizer, protein: bool = False):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "        if protein:\n",
    "            self.diff = 3\n",
    "        else:\n",
    "            self.diff = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence = self.sequences[i]\n",
    "        inputs = self.tokenizer.encode(sequence, return_tensors='pt')\n",
    "        targets = inputs[self.diff:].clone()\n",
    "        inputs = inputs[:-self.diff]\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def compute_char_probabilities(sequences: List[str]) -> None:\n",
    "    counts = defaultdict(int)\n",
    "    total_count = 0\n",
    "    for seq in sequences:\n",
    "        cd = Counter(seq)\n",
    "        for c, count in cd.items():\n",
    "            counts[c] += count\n",
    "            total_count += count\n",
    "    pcounts = {char: c / total_count for char, c in counts.items()}\n",
    "    print(pcounts)\n",
    "\n",
    "def calculate_entropy(sequences, k):\n",
    "    substrings = [seq[i:i+k] for seq in sequences for i in range(len(seq) - k + 1)]\n",
    "    frequencies = Counter(substrings)\n",
    "    entropy = 0.0\n",
    "    total = sum(frequencies.values())\n",
    "    probabilities = {k: v / total for k, v in frequencies.items()}\n",
    "    for p in probabilities.values():\n",
    "        entropy -= p * math.log2(p)\n",
    "    print(probabilities)\n",
    "    print('entropy = ', entropy/k)\n",
    " \n",
    "\n",
    "def read_fna(file_path: str, shuffle: bool = False) -> List[str]:\n",
    "    sequences = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"fasta\"):\n",
    "            sequences.append(str(record.seq))\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_markov_chain(n: int, sparsity: float) -> torch.Tensor:\n",
    "    transition_probs = torch.rand(n, n)\n",
    "    for i in range(n):\n",
    "        num_outgoing_states = torch.poisson(torch.tensor([float(sparsity)])).int().item()\n",
    "        num_outgoing_states = min(n, max(1, num_outgoing_states))\n",
    "        _, indices = torch.topk(transition_probs[i], int(num_outgoing_states))\n",
    "        mask = torch.zeros_like(transition_probs[i]).scatter_(0, indices, 1).to(torch.bool)\n",
    "        transition_probs[i] *= mask\n",
    "    transition_probs = F.normalize(transition_probs, p=1, dim=1)\n",
    "    return transition_probs\n",
    "\n",
    "\n",
    "def draw_seq(transition_probs: torch.Tensor, sequence_length: int) -> str:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    current_state = 0\n",
    "    chain = [current_state]\n",
    "    while len(chain) < sequence_length:\n",
    "        next_state = torch.multinomial(transition_probs[int(current_state)], num_samples=1)\n",
    "        current_state = next_state.item()\n",
    "        chain.append(int(current_state))\n",
    "    seq = [NUCLEOTIDES[s % len(NUCLEOTIDES)] for s in chain]\n",
    "    return ''.join(seq)\n",
    "\n",
    "\n",
    "def generate_HMM_dataset(sequence_length: int, N: int, sparsity: float, num_hidden_states: int) -> List[List[str]]:\n",
    "    if num_hidden_states is None:\n",
    "        num_hidden_states = sequence_length\n",
    "    transition_matrix = generate_markov_chain(num_hidden_states, sparsity=sparsity)\n",
    "    all_seqs = [[], []]\n",
    "    for i in range(2):\n",
    "        seqs = []\n",
    "        while len(seqs) < N:\n",
    "            seq = draw_seq(transition_matrix, sequence_length)\n",
    "            if len(seq) >= sequence_length:\n",
    "                seqs.append(seq)\n",
    "        all_seqs[i] = seqs\n",
    "    return all_seqs\n",
    "\n",
    "\n",
    "def generate_phylo_dataset(sequence_length: int, mutation_rate: float, N: int) -> List[str]:\n",
    "    NUCLEOTIDES = ['A', 'C', 'G', 'T']\n",
    "    parent_sequence = ''.join(random.choice(NUCLEOTIDES) for _ in range(sequence_length))\n",
    "    dataset = [parent_sequence]\n",
    "    for _ in range(N):\n",
    "        mutated_sequence = ''\n",
    "        for nucleotide in parent_sequence:\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_sequence += random.choice(NUCLEOTIDES)\n",
    "            else:\n",
    "                mutated_sequence += nucleotide\n",
    "        dataset.append(mutated_sequence)\n",
    "        parent_sequence = mutated_sequence\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def construct_debruijn_graph(dataset: List[str], k: int) -> DefaultDict[str,Set[str]]:\n",
    "    graph = defaultdict(set)\n",
    "    for sequence in dataset:\n",
    "        for i in range(len(sequence) - k):\n",
    "            graph[sequence[i:i + k]].add(sequence[i + 1:i + k + 1])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def extract_substrings(sequences: List[str], sequence_length: int, stride: int, substrings_per_seq: int, protein: bool = False) -> List[str]:\n",
    "    substrings = []\n",
    "    for sequence in sequences:\n",
    "        if not bool(re.match(\"^[ACGT]+$\", sequence)):\n",
    "            continue\n",
    "        if protein:\n",
    "            sequence = dna2protein(sequence)\n",
    "        for i in range(0, len(sequence) - sequence_length + 1, stride):\n",
    "            if i // stride > substrings_per_seq:\n",
    "                break\n",
    "            seq = sequence[i:i + sequence_length]\n",
    "            substrings.append(seq)\n",
    "    return substrings\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, device: torch.device, description: str, print_every: int) -> float:\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    bar = tqdm(train_loader, desc=description)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for i, (inputs, targets) in enumerate(bar):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs, labels=targets)\n",
    "        # loss = outputs.loss.mean()\n",
    "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "        if (i+1) % print_every == 0:\n",
    "            avg_loss = sum(running_loss) / len(running_loss)\n",
    "            bar.set_postfix({\"Train Loss\": f\"{avg_loss:.5f}\"})\n",
    "            running_loss = []\n",
    "    return sum(running_loss) / len(running_loss)\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, val_loader: DataLoader, device: torch.device, description: str, print_every: int) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    running_loss = []\n",
    "    total_acc = []\n",
    "    bar = tqdm(val_loader, desc=description)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i,(inputs, targets) in enumerate(bar):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, labels=targets)\n",
    "            val_loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            accuracy = (predictions.int() == targets.int()).float().mean().item()\n",
    "            total_loss.append(val_loss.item())\n",
    "            running_loss.append(val_loss.item())\n",
    "            total_acc.append(accuracy)\n",
    "            if (i + 1) % print_every  == 0:\n",
    "                avg_loss = sum(running_loss) / len(running_loss)\n",
    "                bar.set_postfix({\"Val Loss\": f\"{avg_loss:.5f}\", \"Val Accuracy\": accuracy})\n",
    "                running_loss = []\n",
    "                # bar.set_postfix({\"Val Loss\": val_loss.item(), \"Val Accuracy\": accuracy})\n",
    "    avg_val_loss = sum(total_loss) / len(total_loss)\n",
    "    avg_accuracy = sum(total_acc) / len(total_acc)\n",
    "    return avg_val_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def train_loop(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, config: Config) -> None:\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_no_improve = 0  # Number of epochs with no improvement in validation loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(config.num_epochs):\n",
    "            train_loss = train(model, optimizer, train_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Training\", config.print_every)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, device, f\"Epoch {epoch + 1}/{config.num_epochs} | Validation\", config.print_every)\n",
    "            samples = (epoch+1) * len(train_loader) * config.train_bs\n",
    "            print(f\"Epoch {epoch + 1}/{config.num_epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Val Accuracy: {val_acc:.5f}\")\n",
    "            wandb.log({\"epoch\": epoch + 1, \"samples\": samples, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "            \n",
    "            if (epoch + 1) % config.save_model_every == 0:\n",
    "                # Save the model weights as an artifact every 10 epochs\n",
    "                artifact = wandb.Artifact(f\"model_weights\", type='model')\n",
    "                torch.save(model, \"gpt2_dna.pt\")\n",
    "                # torch.save(model.state_dict(), 'gpt2_dna.pt')\n",
    "                artifact.add_file('gpt2_dna.pt')\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                num_epochs_no_improve = 0\n",
    "            else:\n",
    "                num_epochs_no_improve += 1\n",
    "                if num_epochs_no_improve >= config.early_stopping_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}...\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user\")\n",
    "    finally:\n",
    "        # torch.save(model.state_dict(), 'gpt2_dna.pt')\n",
    "        torch.save(model, \"gpt2_dna.pt\")\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(config: Config) -> Tuple[List[str], List[str]]:\n",
    "    if config.dataset == DatasetType.HMM:\n",
    "        train_seqs, val_seqs = generate_HMM_dataset(config.sequence_length, N=config.num_seqs, sparsity=config.sparsity,\n",
    "                                                    num_hidden_states=config.num_hidden_states)\n",
    "    elif config.dataset == DatasetType.VIRAL:\n",
    "        set_random_seed(42)\n",
    "        sequences = read_fna(file_path=config.file_path, shuffle=config.sequences_shuffle)\n",
    "        sequences = sequences[:config.num_seqs]\n",
    "        sub_seqs = extract_substrings(sequences, sequence_length=config.sequence_length, stride=config.stride,\n",
    "                                      substrings_per_seq=config.substrings_per_seq, protein=config.protein)\n",
    "        # compute_char_probabilities(sub_seqs)\n",
    "        train_size = int(len(sub_seqs) * (1 - config.split_ratio))\n",
    "        train_seqs = sub_seqs[:train_size]\n",
    "        val_seqs = sub_seqs[train_size:]\n",
    "        train_seqs = train_seqs[:config.num_seqs]\n",
    "        val_seqs = val_seqs[:config.num_seqs]\n",
    "        calculate_entropy(train_seqs, k=2)\n",
    "        calculate_entropy(train_seqs, k=2)\n",
    "    return train_seqs, val_seqs\n",
    "\n",
    "\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = 'cpu'\n",
    "\n",
    "    tokenizer = SequenceTokenizer(protein=config.protein, )\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer, protein=config.protein)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer, protein=config.protein)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "    name = f\"{config.dataset.name}_{'protein' if config.protein else 'DNA'}__lr={config.lr}_bs={config.train_bs}_n_embed={config.n_embed}_n_head={config.n_head}_n_layer={config.n_layer}_early_stopping={config.early_stopping_patience}_num_seqs={config.num_seqs}_sequence_length={config.sequence_length}_stride={config.stride}\"\n",
    "    wandb.init(project='GPT2_DNA', name=name, config=config)\n",
    "\n",
    "    gpt2_config = GPT2Config(vocab_size=tokenizer.vocab_size,\n",
    "                             n_positions=config.sequence_length,\n",
    "                             n_ctx=config.sequence_length,\n",
    "                             n_embd=config.n_embed,\n",
    "                             n_layer=config.n_layer,\n",
    "                             n_head=config.n_head)\n",
    "\n",
    "    model = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    train_loop(model, optimizer, train_loader, val_loader, device, config)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model\u001b[39m(config: Config) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     train_seqs, val_seqs \u001b[39m=\u001b[39m load_datasets(config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblack/home/amir/Codes/GPT_metagenome/GPT_dBG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "def get_model(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = SequenceTokenizer()\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "\n",
    "    model = torch.load('gpt2_dna.pt')\n",
    "\n",
    "    return model, train_loader, val_loader, device\n",
    "\n",
    "model, train_loader, val_loader, device = get_model(Config())\n",
    "\n",
    "inputs, labels = next(iter(val_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "outputs = model(inputs, labels=labels)\n",
    "\n",
    "predicitions = torch.argmax(outputs.logits, dim=-1)\n",
    "tokenizer = SequenceTokenizer()\n",
    "i = 2\n",
    "acc = (predicitions[i] == labels[i]).float().mean().item()\n",
    "print(acc)\n",
    "x = tokenizer.decode(inputs[i])\n",
    "y = tokenizer.decode(predicitions[i])\n",
    "\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AG': 0.060456456456456455, 'GA': 0.06808208208208208, 'GC': 0.06076976976976977, 'CA': 0.06842842842842843, 'AA': 0.08358058058058059, 'AT': 0.0681901901901902, 'TC': 0.0603003003003003, 'CG': 0.057458458458458456, 'GG': 0.05754654654654655, 'GT': 0.054285285285285284, 'TG': 0.06524224224224225, 'CC': 0.05598198198198198, 'TT': 0.07095595595595595, 'TA': 0.0526036036036036, 'AC': 0.06046046046046046, 'CT': 0.05565765765765766}\n",
      "entropy =  1.9947617887039697\n",
      "{'AG': 0.060456456456456455, 'GA': 0.06808208208208208, 'GC': 0.06076976976976977, 'CA': 0.06842842842842843, 'AA': 0.08358058058058059, 'AT': 0.0681901901901902, 'TC': 0.0603003003003003, 'CG': 0.057458458458458456, 'GG': 0.05754654654654655, 'GT': 0.054285285285285284, 'TG': 0.06524224224224225, 'CC': 0.05598198198198198, 'TT': 0.07095595595595595, 'TA': 0.0526036036036036, 'AC': 0.06046046046046046, 'CT': 0.05565765765765766}\n",
      "entropy =  1.9947617887039697\n",
      "Epoch [1/200],  Train Loss: 1.6878, Val Loss: 1.3884\n",
      "Epoch [2/200],  Train Loss: 1.3879, Val Loss: 1.3919\n",
      "Epoch [3/200],  Train Loss: 1.3903, Val Loss: 1.3909\n",
      "Epoch [4/200],  Train Loss: 1.3881, Val Loss: 1.3844\n",
      "Epoch [5/200],  Train Loss: 1.3888, Val Loss: 1.4017\n",
      "Epoch [6/200],  Train Loss: 1.3882, Val Loss: 1.3880\n",
      "Epoch [7/200],  Train Loss: 1.3902, Val Loss: 1.3925\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 1000\n",
    "        self.stride: int = 1000\n",
    "        self.num_seqs: int = 1000\n",
    "        self.split_ratio: float = 0.5\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.protein: bool = False\n",
    "        # self.sparsity: float = 1.1\n",
    "        # self.num_hidden_states: int = 100\n",
    "        self.train_bs: int = 32\n",
    "        self.val_bs: int = 256\n",
    "        self.n_embed: int = 1024\n",
    "        self.n_layer: int = 3\n",
    "        self.n_head: int = 1\n",
    "        self.lr: float = 0.005\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.print_every: int = 20\n",
    "        self.save_model_every: int = 20\n",
    "\n",
    "# Define the LSTM model\n",
    "class DNAPredictor(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(DNAPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.one_hot = lambda x: torch.functional.F.one_hot(x, num_classes=output_size).float()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        input_seq = self.one_hot(input_seq)\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "    \n",
    "\n",
    "config = Config()\n",
    "train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = SequenceTokenizer()\n",
    "train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.train_bs, drop_last=True,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.val_bs, drop_last=True,shuffle=False)\n",
    "\n",
    "output_size = tokenizer.vocab_size\n",
    "# Initialize the model\n",
    "model = DNAPredictor(tokenizer.vocab_size, config.n_embed, tokenizer.vocab_size, num_layers=config.n_layer).to(device)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(config.num_epochs):\n",
    "    total_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1,outputs.shape[-1]), labels.flatten())\n",
    "        total_loss.append(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = sum(total_loss)/len(total_loss)\n",
    "    # print(f'Epoch [{epoch+1}/{config.num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        for i, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criteria(outputs.view(-1,outputs.shape[-1]), labels.flatten())\n",
    "            total_loss.append(loss.item())\n",
    "        val_loss = sum(total_loss)/len(total_loss)\n",
    "        print(f'Epoch [{epoch+1}/{config.num_epochs}],  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
