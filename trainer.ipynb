{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset: DatasetType = DatasetType.VIRAL\n",
    "        self.model_name: str = 'gpt2'\n",
    "        self.file_path: str = \"./data/viral.1.1.genomic.fna\"\n",
    "        self.sequence_length: int = 400\n",
    "        self.stride: int = 200\n",
    "        self.split_ratio: float = 0.5\n",
    "        self.substrings_per_seq: int = 20\n",
    "        self.num_seqs: int = 10000\n",
    "        self.sequences_shuffle: bool = True\n",
    "        self.train_bs: int = 64\n",
    "        self.val_bs: int = 128\n",
    "        self.n_embed: int = 512\n",
    "        self.n_layer: int = 4\n",
    "        self.n_head: int = 16\n",
    "        self.lr: float = 1e-4\n",
    "        self.weight_decay: float = 0.01\n",
    "        self.num_epochs: int = 200\n",
    "        self.early_stopping_patience: int = 5\n",
    "        self.weight_decay: float = 0.00\n",
    "        self.warmup_steps: int = 10\n",
    "        self.print_every: int = 20\n",
    "        self.logging_steps: int = 20\n",
    "\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for the DNA dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids: Tensor\n",
    "    labels: Tensor\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences: List[str], tokenizer: SequenceTokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence = self.sequences[i]\n",
    "        inputs = self.tokenizer.encode(sequence, return_tensors='pt')\n",
    "        targets = inputs[1:].clone()\n",
    "        inputs = inputs[:-1]\n",
    "        return InputExample(input_ids=inputs, labels=targets)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "def main(config: Config) -> None:\n",
    "    train_seqs, val_seqs = load_datasets(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = SequenceTokenizer()\n",
    "    train_dataset = DNADataset(train_seqs, tokenizer)\n",
    "    val_dataset = DNADataset(val_seqs, tokenizer)\n",
    "\n",
    "    wandb.init(project='GPT2_DNA', name='viruses', config=config)\n",
    "\n",
    "    gpt2_config = GPT2Config(vocab_size=tokenizer.vocab_size,\n",
    "                             n_positions=config.sequence_length,\n",
    "                             n_ctx=config.sequence_length,\n",
    "                             n_embd=config.n_embed,\n",
    "                             n_layer=config.n_layer,\n",
    "                             n_head=config.n_head)\n",
    "\n",
    "    model = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=config.num_epochs,              # total # of training epochs\n",
    "        per_device_train_batch_size=config.train_bs,  # batch size per device during training\n",
    "        per_device_eval_batch_size=config.val_bs,   # batch size for evaluation\n",
    "        warmup_steps=config.warmup_steps,                # number of warmup steps for learning rate scheduler\n",
    "        learning_rate=config.lr,         # learning rate\n",
    "        weight_decay=config.weight_decay,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=config.logging_steps,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        optimizers=(optimizer, None),       # optimizer\n",
    "        gradient_accumulation_steps=2,      # Modify as needed\n",
    "        fp16=True,                          # if your GPU supports mixed precision\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the function to compute metrics \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
